{
  "phase": "Phase 1.8: Hardened Autonomous Loop & Advanced Remediation",
  "phase_goal": "Significantly improve the robustness and self-correction capabilities of the autonomous workflow loop based on real-world failure analysis, incorporating learning from failures, more sophisticated remediation, and better error handling.",
  "success_metrics": [
    "The Workflow Driver successfully handles and remediates common syntax, style, ethical, and test failures autonomously.",
    "Critical errors (like IndentationError) are caught and addressed at the step level before writing.",
    "Automated tests are executed after code modification steps.",
    "The remediation loop successfully fixes >= 85% of tasks that fail initial validation (excluding fundamental design/ambiguity issues).",
    "The Grade Report clearly distinguishes validation execution errors from findings and highlights critical issues.",
    "Detailed failure data is logged and stored for learning and analysis.",
    "The system can attempt to decompose complex tasks into smaller sub-tasks.",
    "Code merging is more robust and less likely to introduce syntax errors.",
    "The system can predict the likelihood of autonomous success for a task.",
    "Phase 1.8 tests achieve >= 95% code coverage for new logic."
  ],
  "tasks": [
    {
      "task_id": "task_1_8_1_pre_fix",
      "priority": "Critical",
      "task_name": "Refine Step Classification to Correctly Identify Research Steps",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [],
      "description": "Modify WorkflowDriver's step classification logic to ensure that plan steps primarily involving 'Research and identify', 'Analyze', or 'Investigate' are not classified as code generation steps, even if they mention target files or code-related keywords as examples. This is a prerequisite to allow the main task_1_8_1 to proceed autonomously."
    },
    {
      "task_id": "task_1_8_0_fix_summarizer_synthesize",
      "priority": "Critical",
      "task_name": "Fix AttributeError in RecursiveSummarizer for synthesize method",
      "status": "Completed",
      "target_file": "src/core/chunking/recursive_summarizer.py",
      "depends_on": [
        "task_1_8_1_pre_fix"
      ],
      "description": "The EnhancedLLMOrchestrator calls self.summarizer.synthesize(), but RecursiveSummarizer does not have this method. Add a synthesize(self, summaries: List[str]) -> str method to RecursiveSummarizer in src/core/chunking/recursive_summarizer.py that joins the list of summaries into a single string. This is needed to unblock code generation in _handle_large_context."
    },
    {
      "task_id": "task_1_8_0_fix_token_allocator",
      "priority": "Critical",
      "task_name": "Adjust TokenAllocator Cost Function for Realistic Allocations",
      "status": "Completed",
      "target_file": "src/core/optimization/adaptive_token_allocator.py, src/core/ethics/constraints.py",
      "depends_on": [
        "task_1_8_0_fix_summarizer_synthesize"
      ],
      "description": "The TokenAllocator's cost function _model_cost in src/core/optimization/adaptive_token_allocator.py has a quadratic term that heavily penalizes token count, resulting in minimal allocations (e.g., 101 tokens). Modify the coefficient of the quadratic term (e.g., change the divisor from 1000.0 to 10000000.0) to allow for more realistic token allocations for code generation tasks. Also, ensure the solver is reset before each allocation call."
    },
    {
      "task_id": "task_1_8_1_unblock_overwrite_fix",
      "priority": "Critical",
      "task_name": "Prevent Placeholder Overwrite of Core Python Files for Conceptual Steps",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_1_pre_fix",
        "task_1_8_0_fix_summarizer_synthesize",
        "task_1_8_0_fix_token_allocator"
      ],
      "description": "Modify the WorkflowDriver's autonomous_loop logic. Specifically, in the 'explicit file writing' branch, add a condition to prevent writing placeholder content to the main Python task_target_file of a task if the plan step is conceptual (e.g., 'define a list', 'analyze requirements') and not an explicit 'create file' or 'generate file' instruction. Also, ensure Python-specific placeholders (#) are used if a placeholder write to a .py file is genuinely intended."
    },
    {
      "task_id": "task_1_8_1_fix_syntax_and_add_tests",
      "priority": "Critical",
      "task_name": "Fix Syntax Error in classify_plan_step and Add Unit Tests",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_1_unblock_overwrite_fix"
      ],
      "description": "The code generated for task_1_8_1 (Enhance Plan Step Identification) included an erroneous trailing line causing a syntax/name error. Remove this line from src/core/automation/workflow_driver.py. Additionally, create comprehensive unit tests for the new classify_plan_step function. Tests should cover code steps, conceptual steps, ambiguous steps, and edge cases. Place tests in tests/test_phase1_8_features.py."
    },
    {
      "task_id": "task_1_8_1",
      "priority": "Critical",
      "task_name": "Enhance Plan Step Identification",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_1_fix_syntax_and_add_tests"
      ],
      "description": "Improve the Workflow Driver's logic to better identify plan steps requiring code modification (imports, constants, definitions, etc.) vs. conceptual steps."
    },
    {
      "task_id": "task_1_8_1b_increase_min_token_alloc",
      "priority": "Critical",
      "task_name": "Increase Minimum Token Allocation per Chunk",
      "status": "Completed",
      "target_file": "src/core/optimization/adaptive_token_allocator.py, src/core/ethics/constraints.py",
      "depends_on": [
        "task_1_8_1"
      ],
      "description": "Modify TokenAllocator and EthicalAllocationPolicy to enforce a higher minimum token count per chunk (e.g., 1000 tokens instead of 100). This is crucial for effective code generation, as the current minimum leads to insufficient token allocation and stalls progress on code-generating tasks. This task addresses the shortcomings of task_1_8_0_fix_token_allocator."
    },
    {
      "task_id": "task_1_8_2b_fix_placeholder_overwrite_for_modification_steps",
      "priority": "Critical",
      "task_name": "Refine Placeholder Write Logic for Main Target Modification",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": [],
      "description": "Modify WorkflowDriver.autonomous_loop to prevent placeholder overwrites on the main Python task_target_file for steps that imply modification rather than creation (e.g., \"insert a block\", \"add logic to method X\"). Such steps, if classified as 'explicit file writing', should be treated as code generation steps or have their placeholder write skipped entirely if they are too vague for direct code generation. This refines task_1_8_1_unblock_overwrite_fix. Add unit tests for this specific logic in tests/test_phase1_8_features.py, covering scenarios where placeholders should and should not be written to the main Python target."
    },
    {
      "task_id": "task_1_8_2c_target_test_file_for_test_writing_steps",
      "priority": "Critical",
      "task_name": "Correctly Target Test Files for Unit Test Generation Steps",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_2b_fix_placeholder_overwrite_for_modification_steps"
      ],
      "description": "Enhance WorkflowDriver.autonomous_loop step processing. If a plan step is for writing unit tests (e.g., contains \"write unit tests for X\", mentions tests/test_*.py, or filepath_from_step points to a test file path), ensure the generated test code is written to the appropriate test file path. This path should be derived from filepath_from_step if valid and test-like, or by convention from task_target_file (e.g., src/A.py -> tests/test_A.py). This determined test file path should override task_target_file for the write operation if task_target_file is not itself a test file. Add unit tests for this file targeting logic in tests/test_phase1_8_features.py."
    },
    {
      "task_id": "task_unblock_log_enhance_1_8_2",
      "priority": "Critical",
      "task_name": "Enhance Logging for Code Generation and Pre-Write Validation",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2c_target_test_file_for_test_writing_steps"
      ],
      "description": "Modify the WorkflowDriver to provide more detailed logging during code generation and pre-write validation. Log failed snippets, LLM prompts, and optimizer input/output (if accessible)."
    },
    {
      "task_id": "task_unblock_retry_limit_1_8_2",
      "priority": "Critical",
      "task_name": "Implement Step-Level Retry Limit and Task Blocking in WorkflowDriver",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_unblock_log_enhance_1_8_2"
      ],
      "description": "Modify the WorkflowDriver to retry failing plan steps up to a limit. If a step fails after exhausting retries, mark the task as 'Blocked' in ROADMAP.json with a reason and move to the next task."
    },
    {
      "task_id": "task_1_8_Z_implement_llm_rate_limiting",
      "priority": "Critical",
      "task_name": "Implement Client-Side Rate Limiting for Gemini API Calls",
      "status": "Completed",
      "target_file": "src/core/llm_orchestration.py",
      "depends_on": [
        "task_unblock_retry_limit_1_8_2"
      ],
      "description": "Modify LLMOrchestrator to add client-side rate limiting for Gemini API calls to prevent 429 errors. 1. Add attributes (`_last_gemini_call_start_time`, `_gemini_call_lock`) to LLMOrchestrator. 2. Implement a method `_apply_gemini_rate_limit` that calculates the necessary sleep duration based on a 10 RPM (6 seconds per request) limit, uses a lock for thread safety, applies `time.sleep` if needed, and updates `_gemini_call_start_time`. 3. Call this method in `_generate_with_retry` before invoking `_gemini_generate`. 4. Add unit tests for the rate limiting logic in `tests/test_llm_orchestration.py` to verify correct sleep timing under various conditions. 5. Ensure logging indicates when rate limiting is active."
    },
    {
      "task_id": "task_1_8_X_fix_multi_target_handling",
      "priority": "Critical",
      "task_name": "Correctly Handle Single Target File From Multi-Target Tasks in CodeGen",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_Z_implement_llm_rate_limiting"
      ],
      "description": "Modify `WorkflowDriver.autonomous_loop` to correctly process tasks with multiple comma-separated `target_file` entries. 1. When a code generation step is identified, and the parent task's `target_file` lists multiple files, implement logic to determine the *actual single target file* for the current plan step. This determination should prioritize explicit file mentions in the step description (e.g., 'modify fileA.py', 'in fileB.py'). If no specific file from the task's list is mentioned in the step, it should default to the first file in the `task_target_file` list and log a warning about the ambiguity. 2. Ensure the determined single file path is used for `_read_file_for_context`, pre-write validation, and `_write_output_file`. 3. Add unit tests for this parsing/determination logic in tests/test_phase1_8_features.py. 4. Add an integration test with a multi-target task and a step modifying one specific file, asserting correct file operations."
    },
    {
      "task_id": "task_1_8_Y_ensure_docstrings_in_codegen",
      "priority": "Critical",
      "task_name": "Ensure Docstrings in CoderLLM Output for Python Code",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_X_fix_multi_target_handling"
      ],
      "description": "Modify `WorkflowDriver.autonomous_loop` prompt construction for the CoderLLM. 1. When a plan step involves Python code generation that is likely to create new functions, methods, or classes, explicitly instruct the CoderLLM in its prompt to include comprehensive docstrings. The instruction should specify that docstrings must explain the purpose, arguments (name, type, description), and return values (type, description). Example instruction: 'IMPORTANT: For any new Python functions, methods, or classes, you MUST include a comprehensive PEP 257 compliant docstring. Use Google-style format (Args:, Returns:, Example: sections). This is required to pass automated ethical and style checks.'. 2. Add unit tests to verify that CoderLLM prompts for Python function/method generation include this instruction in tests/test_phase1_8_features.py."
    },
    {
      "task_id": "task_1_8_fix_rate_limit_enh_orchestrator",
      "priority": "Critical",
      "task_name": "Fix Gemini Rate Limiting in EnhancedLLMOrchestrator",
      "status": "Completed",
      "target_file": "src/core/llm_orchestration.py",
      "depends_on": [
        "task_1_8_Y_ensure_docstrings_in_codegen"
      ],
      "description": "The `_call_llm_api` method in `EnhancedLLMOrchestrator` directly calls `_gemini_generate` without going through `_generate_with_retry`, bypassing the rate limiting logic. Modify `_call_llm_api` to correctly incorporate rate limiting for Gemini calls by directly invoking `self._apply_gemini_rate_limit()` before `self._gemini_generate(text)` if the model is 'gemini'. Add unit tests to verify rate limiting is applied in this specific execution path."
    },
    {
      "task_id": "task_1_8_improve_snippet_handling",
      "priority": "Critical",
      "task_name": "Improve Code Snippet Generation, Validation, and Merging Robustness",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, src/core/llm_orchestration.py",
      "depends_on": [
        "task_1_8_fix_rate_limit_enh_orchestrator"
      ],
      "description": "The system frequently encounters syntax errors (e.g., 'unterminated string literal', 'unexpected indent') during pre-write validation (`ast.parse`) of CoderLLM-generated snippets or after merging. This blocks autonomous development. This task aims to improve robustness: 1. **Enhanced Logging:** (Completed) Modify `WorkflowDriver._invoke_coder_llm` to save the exact `generated_snippet` string (using `repr()`) to a temporary file if `ast.parse` fails, to capture hidden characters/malformations for debugging. 2. **Prompt Refinement:** (Completed) Review and refine CoderLLM prompts in `workflow_driver.py` to explicitly guide the LLM to output syntactically correct, complete, and context-aware code snippets, minimizing issues with string literals, indentation, and partial outputs. 3. **Merge Strategy Review:** (Completed) Modify the `_merge_snippet` method in `workflow_driver.py` to attempt basic indentation adjustment of the snippet if `METAMORPHIC_INSERT_POINT` is found. 4. **Pre-Merge Full File Syntax Check:** (Completed) Before `_merge_snippet` is called, create a temporary in-memory version of the target file content with the snippet hypothetically inserted at the `METAMORPHIC_INSERT_POINT`. Attempt `ast.parse()` on this *full temporary content*. If this full parse fails, the step should fail with specific feedback indicating an integration syntax error, rather than just the snippet failing."
    },
    {
      "task_id": "task_1_8_18_fix_string_literal_prompting",
      "priority": "Critical",
      "task_name": "Harden CoderLLM Prompting to Prevent String Literal Syntax Errors",
      "status": "Completed",
      "target_file": "src/core/constants.py",
      "depends_on": [
        "task_1_8_improve_snippet_handling"
      ],
      "description": "The CoderLLM is repeatedly generating snippets with `SyntaxError: unterminated triple-quoted string literal`. To fix this, modify the `GENERAL_SNIPPET_GUIDELINES` constant in `src/core/constants.py`. Add a new, high-priority rule at the top of the guidelines under a heading like `CRITICAL SYNTAX RULES`. This rule must explicitly instruct the LLM on how to correctly handle string literals to avoid this specific error. Key points to include in the instruction are: 1. All string literals MUST be correctly terminated with matching quotes. 2. Be extremely careful with raw strings (r'...'), as a backslash (`\\`) cannot be the final character. 3. When a string literal must contain code (like a test case or a prompt), ALWAYS use triple quotes (`\"\"\"` or `'''`). Provide clear examples of correct and incorrect usage."
    },
    {
      "task_id": "task_1_8_19a_0_plan_step_bundling",
      "priority": "Critical",
      "task_name": "Implement Code Generation Step Bundling",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [],
      "description": "In `WorkflowDriver.autonomous_loop`, when a code generation step is identified, look ahead at the next `MAX_BUNDLE_LOOKAHEAD` steps. If they are also classified as code generation steps for the same file, bundle their descriptions into a single, more comprehensive prompt for the Coder LLM. Mark the bundled subsequent steps as 'subsumed' and skip their execution. This prevents failures from overly-granular plans."
    },
    {
      "task_id": "task_1_8_19a_1_wrap_ast_parse",
      "priority": "High",
      "task_name": "Wrap ast.parse in a try-except block for SyntaxError",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_18_fix_string_literal_prompting"
      ],
      "description": "In `_execute_code_generation_step`, find the `ast.parse(content_for_ast_check)` call. Wrap this specific call in a `try...except SyntaxError as se:` block. For now, inside the `except` block, simply re-raise the exception using `raise se`. This is the first step to build the differentiation logic, enabling safe detection of syntax errors without crashing the workflow."
    },
    {
      "task_id": "task_1_8_19a_2",
      "priority": "High",
      "task_name": "Implement AST-Aware Syntax Error Differentiation",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_19a_1_wrap_ast_parse"
      ],
      "description": "In `_execute_code_generation_step`, enhance the `except SyntaxError as se:` block. Use AST parsing to programmatically locate the `except` block and provide it as scoped context to the Coder LLM. The LLM will return the entire modified block, which will then be merged. The logic within the block must differentiate between pre-existing and snippet-introduced syntax errors by comparing the error line number to the snippet's hypothetical insertion range."
    },
    {
      "task_id": "task_1_8_19a_4",
      "priority": "High",
      "task_name": "Implement Conditional Logic for Error Differentiation",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_19a_2"
      ],
      "description": "Implement the conditional logic to differentiate between pre-existing and snippet-introduced errors. The condition should check if `snippet_range` is not None and if `error_line_number` is within the `snippet_range` (e.g., `if snippet_range and snippet_range[0] <= error_line_number <= snippet_range[1]:`)."
    },
    {
      "task_id": "task_1_8_19a_5",
      "priority": "High",
      "task_name": "Implement Snippet-Introduced Error Handling",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_19a_4"
      ],
      "description": "Inside the 'if' block (for snippet-introduced errors), log the relevant details (error message, line number, snippet range) and append a descriptive error message to the `validation_feedback` list. Then, re-raise the original `SyntaxError` to trigger the step retry logic."
    },
    {
      "task_id": "task_1_8_19a_6",
      "priority": "High",
      "task_name": "Implement Pre-existing Error Handling",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_19a_4"
      ],
      "description": "Inside the 'else' block (for pre-existing errors), log the relevant details (error message, line number) and raise a `ValueError` with a descriptive message (e.g., 'File already contains syntax errors, cannot proceed with modifications.') to halt execution for the current task and mark it as 'Blocked'. This prevents the system from attempting to modify a file that is already broken."
    },
    {
      "task_id": "task_1_8_19b_add_tests_for_differentiation",
      "priority": "High",
      "task_name": "Add Unit Tests for Syntax Error Differentiation Logic",
      "status": "Completed",
      "target_file": "tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_19a_6"
      ],
      "description": "In `tests/test_phase1_8_features.py`, add new unit tests to verify the syntax error differentiation logic implemented in `task_1_8_19a`. The tests should cover cases where a `SyntaxError` occurs within the generated snippet and cases where the error exists in the original source file content, outside the snippet's range. Use `pytest.raises` and mock `ast.parse` to simulate these `SyntaxError` conditions."
    },
    {
      "task_id": "task_1_8_2_F_4c_add_passing_tests",
      "priority": "High",
      "task_name": "Add Passing Unit Tests for Compliant Snippets",
      "status": "Completed",
      "target_file": "tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_19b_add_tests_for_differentiation"
      ],
      "description": "In `tests/test_phase1_8_features.py`, add another new test method to the `TestContextLeakageValidationUnittest` class. This method MUST use `pytest.mark.parametrize` to test multiple code snippets that *do not* contain any context leakage indicators. For each compliant snippet, the test must assert that `WorkflowDriver._validate_for_context_leakage` correctly returns `True`. Ensure all test case strings are defined using triple quotes (`\"\"\"` or `'''`)."
    },
    {
      "task_id": "task_1_8_2_F_4d_1_create_kg_file_and_class",
      "priority": "High",
      "task_name": "Create Knowledge Graph Protocol File and Class",
      "status": "Completed",
      "target_file": "src/core/llm/knowledge_graph.py",
      "depends_on": [
        "task_1_8_2_F_4c_add_passing_tests"
      ],
      "description": "Create the new file `src/core/llm/knowledge_graph.py`. In this file, define a new class `KnowledgeGraphWithMultiStageCodeGen` that inherits from `BaseKnowledgeGraph`. The class should have a basic `__init__` method that calls `super().__init__()` and accepts a logger. Ensure the file includes the necessary imports (`from .base import BaseKnowledgeGraph`). This is the first step in implementing the 'Multi-Stage Code Generation' protocol."
    },
    {
      "task_id": "task_1_8_2_F_4d_2_add_skeleton_method",
      "priority": "High",
      "task_name": "Implement Protocol Stage 1: Generate Test Method Skeleton",
      "status": "Completed",
      "target_file": "src/core/llm/knowledge_graph.py",
      "depends_on": [
        "task_1_8_2_F_4d_1_create_kg_file_and_class"
      ],
      "description": "Implement the first stage of the 'Multi-Stage Code Generation' protocol. Add a new method `generate_test_method_skeleton` to the `KnowledgeGraphWithMultiStageCodeGen` class. This method should take a `test_method_name` and `function_to_test_name` as arguments and return a string containing a basic Python test function skeleton with a docstring and a `pass` statement."
    },
    {
      "task_id": "task_1_8_2_F_4d_3_add_parametrize_method",
      "priority": "High",
      "task_name": "Implement Protocol Stage 2: Add Parametrize Decorator",
      "status": "Completed",
      "target_file": "src/core/llm/knowledge_graph.py",
      "depends_on": [
        "task_1_8_2_F_4d_2_add_skeleton_method"
      ],
      "description": "Implement the second stage of the 'Multi-Stage Code Generation' protocol. Add a new method `add_parametrize_decorator` to the `KnowledgeGraphWithMultiStageCodeGen` class. This method should take the existing `test_code` (as a string) and a list of `test_cases` as arguments. It should insert the `@pytest.mark.parametrize` decorator above the test function definition."
    },
    {
      "task_id": "task_1_8_2_F_4d_4_add_test_case_gen_method",
      "priority": "High",
      "task_name": "Implement Protocol Stage 3: Generate Individual Test Cases",
      "status": "Completed",
      "target_file": "src/core/llm/knowledge_graph.py",
      "depends_on": [
        "task_1_8_2_F_4d_3_add_parametrize_method"
      ],
      "description": "Implement the third stage of the 'Multi-Stage Code Generation' protocol. Add a new method `generate_individual_test_cases` to the `KnowledgeGraphWithMultiStageCodeGen` class. This method should take a list of `test_cases` and generate the formatted string for the test case data within the `@pytest.mark.parametrize` decorator. The output should be a list of tuples as a string."
    },
    {
      "task_id": "task_1_8_2_F_4d_5_add_validation_logic_method",
      "priority": "High",
      "task_name": "Implement Protocol Stage 4: Add Validation Logic",
      "status": "Completed",
      "target_file": "src/core/llm/knowledge_graph.py",
      "depends_on": [
        "task_1_8_2_F_4d_4_add_test_case_gen_method"
      ],
      "description": "Implement the fourth and final stage of the 'Multi-Stage Code Generation' protocol. Add a new method `add_validation_logic` to the `KnowledgeGraphWithMultiStageCodeGen` class. This method should take the `test_code` string and replace the `pass` statement with the actual test logic using AST transformation for robustness."
    },
    {
      "task_id": "task_1_8_2_F_4d_6_add_tests",
      "priority": "High",
      "task_name": "Add Unit Tests for Multi-Stage Code Generation Protocol",
      "status": "Completed",
      "target_file": "tests/test_knowledge_graph_protocols.py",
      "depends_on": [
        "task_1_8_2_F_4d_5_add_validation_logic_method"
      ],
      "description": "Create a new test file `tests/test_knowledge_graph_protocols.py` and add unit tests for the new `KnowledgeGraphWithMultiStageCodeGen` class and its methods. Ensure each stage of the protocol is tested, including edge cases, to verify its correctness."
    },
    {
      "task_id": "task_1_8_2_I_part1_failure_tracking",
      "priority": "High",
      "task_name": "Implement Failure Tracking Mechanism",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_F_4d_6_add_tests"
      ],
      "description": "In `WorkflowDriver`, add an instance attribute `self.task_failure_counts = {}` in the `__init__` method to track task failures. Use task IDs as keys and integers (count of consecutive failures) as values. This counter should be reset to 0 for a task upon its successful completion."
    },
    {
      "task_id": "task_1_8_2_I_part2_failure_detection",
      "priority": "High",
      "task_name": "Increment Failure Count on Step Error",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_I_part1_failure_tracking"
      ],
      "description": "In `WorkflowDriver.autonomous_loop`, modify the step failure handling logic. When a step fails after exhausting all retries (i.e., when the task is about to be marked 'Blocked'), increment the failure counter for the current task ID in `self.task_failure_counts`."
    },
    {
      "task_id": "task_1_8_2_I_part3_decomposition_logic",
      "priority": "High",
      "task_name": "Implement Decomposition Recommendation Logic",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_I_part2_failure_detection"
      ],
      "description": "Enhance the failure handling in `WorkflowDriver.autonomous_loop`. After a task is marked 'Blocked', if its failure count in `self.task_failure_counts` is 3 or more, log a specific, prominent warning: 'DECOMPOSITION RECOMMENDED: Task [task_id] has failed repeatedly. Consider decomposing it into smaller, sequential sub-tasks in the roadmap.'"
    },
    {
      "task_id": "task_1_8_2_I_part4_add_tests",
      "priority": "High",
      "task_name": "Add Unit Tests for Failure-Driven Decomposition",
      "status": "Not Started",
      "target_file": "tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_2_I_part1_failure_tracking",
        "task_1_8_2_I_part2_failure_detection",
        "task_1_8_2_I_part3_decomposition_logic"
      ],
      "description": "In `tests/test_phase1_8_features.py`, add unit tests to validate the new failure handling logic. Scenarios to test must include: 1. A task failing 3 times, verifying the counter increments and the 'DECOMPOSITION RECOMMENDED' log is triggered. 2. A task succeeding after 2 failures, verifying its counter is reset to 0. 3. Multiple tasks failing independently, verifying their counts are tracked separately."
    },
    {
      "task_id": "task_1_8_2_G_atomic_planner",
      "priority": "Critical",
      "task_name": "Implement Atomic Step Recognition in Planner",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_F_4c_add_passing_tests"
      ],
      "description": "Enhance the planning agent to recognize when a task description requires multiple code changes to be performed in a single, atomic step (e.g., using keywords like 'in a single step', 'atomically', 'together'). When detected, the planner should generate a single, combined plan step for the Coder LLM, rather than breaking it into multiple tiny steps like 'insert line', 'indent line', etc. This prevents intermediate validation failures (e.g., 'F401: imported but unused' as seen in task_1_8_2_F_4a_create_test_class) and ensures logical changes are applied together."
    },
    {
      "task_id": "task_1_8_2_H_lint_codebase",
      "priority": "Critical",
      "task_name": "Codebase Linting and Style Conformance",
      "status": "Not Started",
      "target_file": "src/, tests/",
      "depends_on": [
        "task_1_8_2_G_atomic_planner"
      ],
      "description": "Run a linter (e.g., `flake8`) and an autoformatter (e.g., `black`) across the entire `src` and `tests` directories to fix existing style violations (PEP8, etc.). This will reduce noise in future validation steps, allowing the autonomous system to more accurately identify new issues introduced by its own code modifications. This task should be completed before proceeding with further feature development."
    },
    {
      "task_id": "task_1_8_20_scoped_validation",
      "priority": "High",
      "task_name": "Implement Scoped Validation for Code Review and Ethics",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py,src/core/agents/code_review_agent.py",
      "depends_on": [
        "task_1_8_2_H_lint_codebase"
      ],
      "description": "Modify the validation logic in the WorkflowDriver to perform 'scoped' analysis. Instead of validating the entire file after a change, generate a diff between the original file content and the proposed merged content, and run linters (Flake8) and ethical checks only on the added/modified lines. This prevents the system from being blocked by pre-existing issues in a file and provides more targeted feedback to the Coder LLM."
    },
    {
      "task_id": "task_1_8_C_1_analyze_A1_failure_and_improve_codegen",
      "priority": "High",
      "task_name": "Analyze task_1_8_A_1 Failures & Improve CodeGen",
      "status": "Not Started",
      "target_file": "src/core/llm_orchestration.py",
      "depends_on": [],
      "description": "Analyze the failures encountered during the initial autonomous attempt of task_1_8_A_1 (_is_simple_addition_plan_step). Specifically, investigate why the CoderLLM failed to: 1. Correctly include `import re`. 2. Adhere to style guidelines (E302, E501). 3. Consistently include docstrings for new methods. 4. Avoided outputting spurious code that was appended to files. Implement improvements to CoderLLM prompting, snippet cleaning, or pre-write validation to prevent such issues in the future. This task focuses on the AI improving its own code generation process based on past failures."
    },
    {
      "task_id": "task_1_8_C_2_focus_instruction",
      "priority": "Critical",
      "task_name": "Add Focus Instruction to CoderLLM Prompt for New Code Blocks",
      "status": "Not Started",
      "target_file": "src/core/constants.py, src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_F_4c_add_passing_tests"
      ],
      "description": "To prevent the CoderLLM from getting distracted by large context files when generating new, self-contained code blocks (like methods or classes), update the prompt construction logic. Modify the `CRITICAL_CODER_LLM_FULL_BLOCK_OUTPUT_INSTRUCTIONS` constant in `src/core/constants.py` to include a strong instruction for the LLM to focus *only* on the specific plan step, not the overall task description. Then, update `_construct_coder_llm_prompt` in `workflow_driver.py` to conditionally include this new instruction when generating full blocks."
    },
    {
      "task_id": "task_1_8_3",
      "priority": "Critical",
      "task_name": "Implement Step-Level Remediation Loop",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [],
      "description": "If pre-write validation fails, provide targeted feedback to the Coder LLM and retry generating the snippet for that specific step (e.g., 2-3 attempts) before failing the step or task."
    },
    {
      "task_id": "task_1_8_4",
      "priority": "Critical",
      "task_name": "Ensure Post-Write, Step-Level Test Execution",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_3"
      ],
      "description": "Modify the Workflow Driver to automatically trigger relevant tests (using execute_tests) after any plan step that successfully modifies code, especially if the task has a target_file that implies testability (e.g., .py file). Capture results in _current_task_results."
    },
    {
      "task_id": "task_1_8_5",
      "priority": "High",
      "task_name": "Implement Learning from Failures (Data Capture)",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_4"
      ],
      "description": "Log detailed information about validation failures, remediation attempts, LLM prompts/responses, and outcomes. Store this structured data in the Knowledge Graph or a dedicated database for analysis and learning."
    },
    {
      "task_id": "task_1_8_14",
      "priority": "High",
      "task_name": "Address Ethical Debt in Token Allocator Policy",
      "status": "Not Started",
      "target_file": "src/core/ethics/constraints.py, src/core/optimization/adaptive_token_allocator.py",
      "depends_on": [
        "task_1_8_5"
      ],
      "description": "The temporary fix for AllocationError in EthicalAllocationPolicy bypassed diversity constraints. Implement a robust, ethical token allocation strategy that correctly handles large prompts and multiple chunks, potentially revising Z3 constraints or exploring alternative allocation methods. Ensure the policy promotes model diversity where feasible without causing allocation failures."
    },
    {
      "task_id": "task_1_8_7",
      "priority": "Medium",
      "task_name": "Implement Automated Task Decomposition",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_1",
        "task_1_8_5"
      ],
      "description": "Add logic to the Workflow Driver or a Planning Agent to assess task complexity and automatically break down large tasks into smaller, dependent sub-tasks in the roadmap if needed. As a reference implementation, analyze the manual decomposition of `task_1_8_19a` into `task_1_8_19a_1` through `task_1_8_19a_5`, which breaks down a complex code modification into atomic, verifiable steps. This decomposition reduces LLM cognitive load and improves retry effectiveness."
    },
    {
      "task_id": "task_1_8_8",
      "priority": "Medium",
      "task_name": "Refine Grade Report & Error Logging",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_5"
      ],
      "description": "Update generate_grade_report to clearly distinguish validation execution errors from findings and highlight critical issues. Improve overall logging clarity for debugging autonomous runs."
    },
    {
      "task_id": "task_1_8_9",
      "priority": "High",
      "task_name": "Implement Advanced Code Merging",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_3"
      ],
      "description": "Replace the current string-based _merge_snippet with an AST-aware merging utility to handle code modifications more robustly and reduce the chance of introducing syntax errors during merging."
    },
    {
      "task_id": "task_1_8_10",
      "priority": "High",
      "task_name": "Implement Prompt Self-Correction Mechanism",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_3",
        "task_1_8_6"
      ],
      "description": "If remediation attempts fail for a step, analyze the failure and modify the *prompt* sent to the Coder LLM for the next attempt, or potentially modify the original code generation prompt template based on failure patterns."
    },
    {
      "task_id": "task_1_8_11_auto_refinement",
      "priority": "High",
      "task_name": "Implement Self-Healing Task Descriptions via Refinement Agent",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_10"
      ],
      "description": "Implement a mechanism for the system to self-correct by refining its own roadmap. When a task is marked 'Blocked' due to repeated step failures (e.g., `SyntaxError` during code generation), a new 'Refinement Agent' should be invoked. This agent will analyze the blocked task's description, the failing step, and the final error message (reason_blocked). It will then generate a new, more detailed and prescriptive description for the task and patch the ROADMAP.json file. This allows the system to learn from its failures at a strategic level, not just a code-generation level. The agent should leverage error-specific templates (e.g., for syntax errors, add explicit formatting rules to the description like preferring raw triple-quoted strings). **The Refinement Agent should be triggered after a task has failed 3 consecutive times due to code generation issues.**"
    },
    {
      "task_id": "task_1_8_11",
      "priority": "High",
      "task_name": "Improve Coder LLM Prompt Generation Logic",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_5",
        "task_1_8_10"
      ],
      "description": "Refine how the Workflow Driver constructs prompts for the Coder LLM, incorporating lessons learned from failure data (task_1_8_5) and successful prompt self-correction (task_1_8_10)."
    },
    {
      "task_id": "task_1_8_12",
      "priority": "Medium",
      "task_name": "Implement Task Success Prediction",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_5"
      ],
      "description": "Develop a simple model (potentially using data from task_1_8_5) to predict the likelihood of autonomous success for a given task based on its characteristics. Use this to inform task selection or flag low-probability tasks for manual review."
    },
    {
      "task_id": "task_1_8_13",
      "priority": "High",
      "task_name": "Add Comprehensive Tests for Phase 1.8 Features",
      "status": "Not Started",
      "target_file": "tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_1",
        "task_1_8_3",
        "task_1_8_4",
        "task_1_8_5",
        "task_1_8_7",
        "task_1_8_8",
        "task_1_8_9",
        "task_1_8_10",
        "task_1_8_11",
        "task_1_8_12"
      ],
      "description": "Write comprehensive unit/integration tests covering all new features in Phase 1.8, including pre-write validation, step-level retries, post-write test execution, improved remediation logic, failure data capture, task decomposition, advanced merging, prompt self-correction, improved prompt generation, and task success prediction."
    },
    {
      "task_id": "task_1_8_15_plan_complexity_detection",
      "priority": "High",
      "task_name": "Implement Plan Complexity Detection and Re-Planning",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_7",
        "task_1_8_5"
      ],
      "description": "Develop and integrate a mechanism within the WorkflowDriver to analyze the complexity of generated solution plans. If a plan for a simple task (e.g., short description, few action verbs) is excessively long or contains too many granular steps, the system should flag it and trigger a re-planning attempt with a refined prompt, guiding the planner towards simpler, more atomic plans.",
      "success_criteria": "The system can accurately identify overly complex plans for simple tasks and successfully trigger re-planning, resulting in more concise and effective plans."
    },
    {
      "task_id": "task_1_8_16_context_scoping_refinement",
      "priority": "Not Started",
      "task_name": "Refine Coder LLM Context Scoping for Atomic Modifications",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_5"
      ],
      "description": "Enhance the `WorkflowDriver`'s `_construct_coder_llm_prompt` and `_extract_targeted_context` methods. For plan steps identified as very small, atomic code modifications (e.g., inserting a single line, changing a variable initialization), the system should provide the Coder LLM with a highly localized context (e.g., 5-10 lines around the insertion/modification point) instead of the entire file. This aims to reduce LLM hallucination and improve the precision of code generation for minor changes.",
      "success_criteria": "The Coder LLM receives appropriately scoped context for atomic modifications, leading to fewer hallucinations and higher success rates for small code changes. Unit tests verify correct context extraction for various atomic modification scenarios."
    },
    {
      "task_id": "task_1_8_17_robust_step_classification",
      "priority": "Not Started",
      "task_name": "Implement Robust Plan Step Classification (AST/Semantic)",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_5"
      ],
      "description": "Replace or significantly enhance the current regex-based plan step classification in `WorkflowDriver` with a more robust, semantic approach (e.g., AST parsing of step descriptions, NLP semantic understanding, or a fine-tuned model). This aims to accurately identify the intent of each step (e.g., 'add constant', 'modify method', 'refactor block') to ensure precise context is provided to the Coder LLM, reducing hallucinations and improving code generation accuracy."
    },
    {
      "task_id": "task_1_8_18_pre_write_diff_validation",
      "priority": "Not Started",
      "task_name": "Implement Pre-Write Diff Validation Guardrail",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_improve_snippet_handling",
        "task_1_8_5"
      ],
      "description": "Introduce a new pre-write validation step in `WorkflowDriver.autonomous_loop`. Before writing the merged code, generate a diff between the original file content and the proposed merged content. If the diff indicates unintended modifications (e.g., changes outside the targeted area for a 'simple addition' task, or excessive unrelated lines), reject the change and provide specific feedback to the Coder LLM for remediation. This acts as a crucial guardrail against LLM over-generation or misinterpretation."
    },
    {
      "task_id": "task_1_8_21_raw_string_validation",
      "priority": "High",
      "task_name": "Implement Targeted Raw String Literal Validation",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_improve_snippet_handling"
      ],
      "description": "Enhance pre-write validation in `WorkflowDriver` with a targeted regex check to specifically detect malformed raw string literals (e.g., `r\"\"\"`) in generated Python code. This acts as an immediate guardrail to catch common LLM syntax errors related to string escaping before `ast.parse`. Example regex check: `if re.search(r'\\br[\"\\'][\"\\']{2}', code_snippet): raise ValueError('Invalid raw string')`."
    },
    {
      "task_id": "task_1_8_22_roadmap_example_style_guide",
      "priority": "Medium",
      "task_name": "Update CONTRIBUTING.md for ROADMAP.json Example Style",
      "status": "Not Started",
      "target_file": "CONTRIBUTING.md",
      "depends_on": [],
      "description": "Update the `CONTRIBUTING.md` file to include a style guide rule for providing Python code examples within `ROADMAP.json` descriptions. The rule should recommend preferring raw triple-single-quoted strings (`r'''...'''`) or raw triple-double-quoted strings (`r\"\"\"...\"\"\"`) for multi-line code blocks, or careful escaping with single-quoted JSON strings (`'...'`) to avoid ambiguity and `SyntaxError` for LLMs. Example: `\u2705 r'''class Example:\\n    pass'''` vs. `\u274c \"class Example:\\n    pass\"`."
    },
    {
      "task_id": "task_1_8_23_failing_test_success_criteria",
      "priority": "High",
      "task_name": "Implement Success Criteria for Intentionally Failing Tests",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_4"
      ],
      "description": "Enhance the `_parse_and_evaluate_grade_report` method to handle tasks that are expected to produce failing tests. Add a field to the task definition in `ROADMAP.json` (e.g., `\"expected_outcome\": \"tests_fail\"`). If this field is present, the grading logic should treat a 'failed' test status as a success for that specific task, allowing the system to proceed with Test-Driven Development workflows autonomously."
    },
    {
      "task_id": "task_1_8_24_atomic_plan_generation",
      "priority": "High",
      "task_name": "Improve Planner for Atomic Code Modifications",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_19b_add_tests_for_differentiation"
      ],
      "description": "The failure of `task_1_8_19a_1_wrap_ast_parse` revealed that the planner can generate overly granular, error-prone plans for simple, atomic code modifications (like wrapping a line in a try/except block). Enhance the planning agent to recognize when a task description implies a single, localized change. For such tasks, the planner should generate a single, more descriptive plan step that instructs the Coder LLM to perform the entire modification at once, rather than breaking it into multiple tiny steps like 'insert line', 'indent line', etc. This will improve the reliability and efficiency of code generation for simple refactoring and additions."
    },
    {
      "task_id": "task_1_8_11b_failure_driven_decomposition",
      "priority": "High",
      "task_name": "Implement Failure-Driven Task Decomposition",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_11_auto_refinement"
      ],
      "description": "Enhance the Refinement Agent (from task_1_8_11_auto_refinement). When a task is blocked because a plan step failed all retries, and that step is not the last in the plan, the agent should attempt to decompose the task. It will split the original task into two: one containing the successful steps up to (but not including) the failure point, which will be marked as 'Completed'. The second, new task will be a copy of the original but with its plan starting from the failing step, and it will depend on the first part. This allows the system to make incremental progress on complex tasks that fail midway through their plan."
    },
    {
      "task_id": "task_1_8_25_learn_from_manual_fixes",
      "priority": "Medium",
      "task_name": "Implement Learning from Manual Fixes",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py,src/core/agents/refinement_agent.py",
      "depends_on": [
        "task_1_8_5",
        "task_1_8_8",
        "task_1_8_11_auto_refinement"
      ],
      "description": "When a task is 'Blocked' and then manually moved to 'Completed' via a git commit, trigger a new learning workflow. This workflow will perform a `git diff` to extract the human-provided code change. It will then analyze the diff in conjunction with the original task description and failure logs. The resulting analysis (e.g., 'the human added a try-except block to handle this error') will be stored in the Knowledge Graph as a successful remediation pattern for future reference by the planning and remediation agents.",
      "effort_estimate": "5 points",
      "prerequisites": [
        "task_1_8_5",
        "task_1_8_8",
        "task_1_8_11_auto_refinement"
      ]
    }
  ],
  "next_phase_actions": [
    "Set `status`: `Completed` on all Phase 1.8 tasks.",
    "Update the `phase`, `phase_goal`, and `current_focus` fields to 'Phase 2 Iteration 2: Enhanced Agents & Knowledge Graph'."
  ],
  "current_focus": "🎯 CURRENT FOCUS: Phase 1.8 - Hardened Autonomous Loop & Advanced Remediation 🛠️"
}