# tests/test_adversarial.py
import unittest
import pytest
from hypothesis import given, strategies as st
from src.core.llm_orchestration import EnhancedLLMOrchestrator, FormalVerificationError, CriticalFailure
from src.core.chunking.dynamic_chunker import CodeChunk
from unittest.mock import MagicMock, patch

class TestAdversarialHandling(unittest.TestCase):
    def setUp(self):
        self.orchestrator = EnhancedLLMOrchestrator(
            kg=MagicMock(),
            spec=MagicMock(),
            ethics_engine=MagicMock()
        )

    @patch.object(EnhancedLLMOrchestrator, '_gemini_generate') # Mock Gemini API call
    @patch.object(EnhancedLLMOrchestrator, '_hf_generate') # Mock HF API call
    @given(st.text(min_size=5000))  # Reduced from 100000
    @patch.object(EnhancedLLMOrchestrator, '_count_tokens', return_value=5001) # Mock token count to trigger large context
    def test_adversarial_inputs_large_payload(self, mock_count_tokens, mock_hf_generate, mock_gemini_generate, payload):
        """Test handling of large input payloads."""
        mock_gemini_generate.return_value = "Mock response" # Dummy response
        mock_hf_generate.return_value = "Mock response"
        self.orchestrator.spec.validate_chunks.return_value = False # make chunk validation fail
        with pytest.raises(CriticalFailure) as excinfo: # Use pytest.raises context manager
            self.orchestrator.generate(payload)
        assert isinstance(excinfo.value, CriticalFailure)

    @patch.object(EnhancedLLMOrchestrator, '_gemini_generate') # Mock Gemini API call
    @patch.object(EnhancedLLMOrchestrator, '_hf_generate') # Mock HF API call
    @given(st.text(min_size=5000, max_size=10000))
    def test_long_unicode_payloads(self, mock_hf_generate, mock_gemini_generate, payload):
        """Test robustness against long unicode payloads."""
        mock_gemini_generate.return_value = "Mock response" # Dummy response
        mock_hf_generate.return_value = "Mock response"
        long_unicode_payload = payload + "ðŸ”¥" * 500
        self.orchestrator.spec.verify_predictions.return_value = {'verified': False} # Mock spec verify_predictions to fail
        with pytest.raises(FormalVerificationError) as excinfo: # Use pytest.raises context manager
            self.orchestrator.generate(long_unicode_payload)
        assert isinstance(excinfo.value, FormalVerificationError)

    @pytest.mark.skip(reason="Ethical validation needs to be fully implemented - skipping until then")
    def test_ethical_violation_detection(self): # Needs proper ethical checks
        """Test detection of ethically problematic code chunks."""
        toxic_chunk = CodeChunk(content=self._generate_hate_speech()) # Create CodeChunk instance
        with pytest.raises(Exception): # Replace EthicalViolation with Exception for now
            self.orchestrator._process_chunk(toxic_chunk, (1000, 'gemini')) # Mock allocation and call _process_chunk directly

    def _generate_hate_speech(self): # Dummy hate speech generator
        return "// Malicious content\n" + "\n".join(f"phrase_{i}" for i in range(100))

    @patch('src.core.llm_orchestration.EnhancedLLMOrchestrator._recursive_summarization_strategy')
    @patch('src.core.llm_orchestration.EnhancedLLMOrchestrator._third_model')
    @patch('src.core.llm_orchestration.EnhancedLLMOrchestrator._secondary_model')
    @patch('src.core.llm_orchestration.EnhancedLLMOrchestrator._primary_processing', side_effect=Exception("Primary failed")) # Force primary to fail
    @patch.object(EnhancedLLMOrchestrator, '_gemini_generate') # Mock Gemini API call
    @patch.object(EnhancedLLMOrchestrator, '_hf_generate') # Mock HF API call
    def test_fallback_strategy_called_adversarial(self, mock_hf_generate, mock_gemini_generate, mock_primary, mock_secondary, mock_third, mock_recursive):
        """Test fallback strategies are engaged."""
        mock_gemini_generate.return_value = "Mock response" # Dummy response
        mock_hf_generate.return_value = "Mock response"
        orchestrator = EnhancedLLMOrchestrator(
            kg=MagicMock(),
            spec=MagicMock(),
            ethics_engine=MagicMock()
        )
        prompt = "Craft code that will intentionally fail verification"
        with pytest.raises(FormalVerificationError) as excinfo: # Use pytest.raises context manager
            orchestrator.generate(prompt)
        assert isinstance(excinfo.value, FormalVerificationError)
        mock_primary.assert_called_once()
        mock_secondary.assert_called_once()
        mock_third.assert_called_once()
        mock_recursive.assert_called_once()

    @patch('src.core.llm_orchestration.EnhancedLLMOrchestrator._recursive_summarization_strategy', side_effect=Exception("Recursive failed"))
    @patch('src.core.llm_orchestration.EnhancedLLMOrchestrator._third_model', side_effect=Exception("Third failed"))
    @patch('src.core.llm_orchestration.EnhancedLLMOrchestrator._secondary_model', side_effect=Exception("Secondary failed"))
    @patch('src.core.llm_orchestration.EnhancedLLMOrchestrator._primary_processing', side_effect=Exception("Primary failed"))
    @patch.object(EnhancedLLMOrchestrator, '_gemini_generate') # Mock Gemini API call
    @patch.object(EnhancedLLMOrchestrator, '_hf_generate') # Mock HF API call
    def test_fallback_strategy_failure_critical_adversarial(self, mock_hf_generate, mock_gemini_generate, mock_primary, mock_secondary, mock_third, mock_recursive):
        """Confirm critical failure on strategy exhaustion."""
        mock_gemini_generate.return_value = "Mock response" # Dummy response
        mock_hf_generate.return_value = "Mock response"
        orchestrator = EnhancedLLMOrchestrator(
            kg=MagicMock(),
            spec=MagicMock(),
            ethics_engine=MagicMock()
        )
        prompt = "Provoke a critical failure"
        with pytest.raises(CriticalFailure) as excinfo: # Use pytest.raises context manager
            orchestrator.generate(prompt)
        assert isinstance(excinfo.value, CriticalFailure)
        assert "All processing strategies failed" in str(excinfo.value)

