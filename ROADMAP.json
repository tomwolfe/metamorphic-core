{
  "phase": "Phase 1.8: Hardened Autonomous Loop & Advanced Remediation",
  "phase_goal": "Significantly improve the robustness and self-correction capabilities of the autonomous workflow loop based on real-world failure analysis, incorporating learning from failures, more sophisticated remediation, and better error handling.",
  "success_metrics": [
    "The Workflow Driver successfully handles and remediates common syntax, style, ethical, and test failures autonomously.",
    "Critical errors (like IndentationError) are caught and addressed at the step level before writing.",
    "Automated tests are executed after code modification steps.",
    "The remediation loop successfully fixes >= 85% of tasks that fail initial validation (excluding fundamental design/ambiguity issues).",
    "The Grade Report clearly distinguishes validation execution errors from findings and highlights critical issues.",
    "Detailed failure data is logged and stored for learning and analysis.",
    "The system can attempt to decompose complex tasks into smaller sub-tasks.",
    "Code merging is more robust and less likely to introduce syntax errors.",
    "The system can predict the likelihood of autonomous success for a task.",
    "Phase 1.8 tests achieve >= 95% code coverage for new logic."
  ],
  "tasks": [
    {
      "task_id": "task_1_8_1_pre_fix",
      "priority": "Critical",
      "task_name": "Refine Step Classification to Correctly Identify Research Steps",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [],
      "description": "Modify WorkflowDriver's step classification logic to ensure that plan steps primarily involving 'Research and identify', 'Analyze', or 'Investigate' are not classified as code generation steps, even if they mention target files or code-related keywords as examples. This is a prerequisite to allow the main task_1_8_1 to proceed autonomously."
    },
    {
      "task_id": "task_1_8_0_fix_summarizer_synthesize",
      "priority": "Critical",
      "task_name": "Fix AttributeError in RecursiveSummarizer for synthesize method",
      "status": "Completed",
      "target_file": "src/core/chunking/recursive_summarizer.py",
      "depends_on": [
        "task_1_8_1_pre_fix"
      ],
      "description": "The EnhancedLLMOrchestrator calls self.summarizer.synthesize(), but RecursiveSummarizer does not have this method. Add a synthesize(self, summaries: List[str]) -> str method to RecursiveSummarizer in src/core/chunking/recursive_summarizer.py that joins the list of summaries into a single string. This is needed to unblock code generation in _handle_large_context."
    },
    {
      "task_id": "task_1_8_0_fix_token_allocator",
      "priority": "Critical",
      "task_name": "Adjust TokenAllocator Cost Function for Realistic Allocations",
      "status": "Completed",
      "target_file": "src/core/optimization/adaptive_token_allocator.py, src/core/ethics/constraints.py",
      "depends_on": [
        "task_1_8_0_fix_summarizer_synthesize"
      ],
      "description": "The TokenAllocator's cost function _model_cost in src/core/optimization/adaptive_token_allocator.py has a quadratic term that heavily penalizes token count, resulting in minimal allocations (e.g., 101 tokens). Modify the coefficient of the quadratic term (e.g., change the divisor from 1000.0 to 10000000.0) to allow for more realistic token allocations for code generation tasks. Also, ensure the solver is reset before each allocation call."
    },
    {
      "task_id": "task_1_8_1_unblock_overwrite_fix",
      "priority": "Critical",
      "task_name": "Prevent Placeholder Overwrite of Core Python Files for Conceptual Steps",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_1_pre_fix",
        "task_1_8_0_fix_summarizer_synthesize",
        "task_1_8_0_fix_token_allocator"
      ],
      "description": "Modify the WorkflowDriver's autonomous_loop logic. Specifically, in the 'explicit file writing' branch, add a condition to prevent writing placeholder content to the main Python task_target_file of a task if the plan step is conceptual (e.g., 'define a list', 'analyze requirements') and not an explicit 'create file' or 'generate file' instruction. Also, ensure Python-specific placeholders (#) are used if a placeholder write to a .py file is genuinely intended."
    },
    {
      "task_id": "task_1_8_1_fix_syntax_and_add_tests",
      "priority": "Critical",
      "task_name": "Fix Syntax Error in classify_plan_step and Add Unit Tests",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_1_unblock_overwrite_fix"
      ],
      "description": "The code generated for task_1_8_1 (Enhance Plan Step Identification) included an erroneous trailing line causing a syntax/name error. Remove this line from src/core/automation/workflow_driver.py. Additionally, create comprehensive unit tests for the new classify_plan_step function. Tests should cover code steps, conceptual steps, ambiguous steps, and edge cases. Place tests in tests/test_phase1_8_features.py."
    },
    {
      "task_id": "task_1_8_1",
      "priority": "Critical",
      "task_name": "Enhance Plan Step Identification",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_1_fix_syntax_and_add_tests"
      ],
      "description": "Improve the Workflow Driver's logic to better identify plan steps requiring code modification (imports, constants, definitions, etc.) vs. conceptual steps."
    },
    {
      "task_id": "task_1_8_1b_increase_min_token_alloc",
      "priority": "Critical",
      "task_name": "Increase Minimum Token Allocation per Chunk",
      "status": "Completed",
      "target_file": "src/core/optimization/adaptive_token_allocator.py, src/core/ethics/constraints.py",
      "depends_on": [
        "task_1_8_1"
      ],
      "description": "Modify TokenAllocator and EthicalAllocationPolicy to enforce a higher minimum token count per chunk (e.g., 1000 tokens instead of 100). This is crucial for effective code generation, as the current minimum leads to insufficient token allocation and stalls progress on code-generating tasks. This task addresses the shortcomings of task_1_8_0_fix_token_allocator."
    },
    {
      "task_id": "task_1_8_2b_fix_placeholder_overwrite_for_modification_steps",
      "priority": "Critical",
      "task_name": "Refine Placeholder Write Logic for Main Target Modification",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": [],
      "description": "Modify WorkflowDriver.autonomous_loop to prevent placeholder overwrites on the main Python task_target_file for steps that imply modification rather than creation (e.g., \"insert a block\", \"add logic to method X\"). Such steps, if classified as 'explicit file writing', should be treated as code generation steps or have their placeholder write skipped entirely if they are too vague for direct code generation. This refines task_1_8_1_unblock_overwrite_fix. Add unit tests for this specific logic in tests/test_phase1_8_features.py, covering scenarios where placeholders should and should not be written to the main Python target."
    },
    {
      "task_id": "task_1_8_2c_target_test_file_for_test_writing_steps",
      "priority": "Critical",
      "task_name": "Correctly Target Test Files for Unit Test Generation Steps",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_2b_fix_placeholder_overwrite_for_modification_steps"
      ],
      "description": "Enhance WorkflowDriver.autonomous_loop step processing. If a plan step is for writing unit tests (e.g., contains \"write unit tests for X\", mentions tests/test_*.py, or filepath_from_step points to a test file path), ensure the generated test code is written to the appropriate test file path. This path should be derived from filepath_from_step if valid and test-like, or by convention from task_target_file (e.g., src/A.py -> tests/test_A.py). This determined test file path should override task_target_file for the write operation if task_target_file is not itself a test file. Add unit tests for this file targeting logic in tests/test_phase1_8_features.py."
    },
    {
      "task_id": "task_unblock_log_enhance_1_8_2",
      "priority": "Critical",
      "task_name": "Enhance Logging for Code Generation and Pre-Write Validation",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2c_target_test_file_for_test_writing_steps"
      ],
      "description": "Modify the WorkflowDriver to provide more detailed logging during code generation and pre-write validation. Log failed snippets, LLM prompts, and optimizer input/output (if accessible)."
    },
    {
      "task_id": "task_unblock_retry_limit_1_8_2",
      "priority": "Critical",
      "task_name": "Implement Step-Level Retry Limit and Task Blocking in WorkflowDriver",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_unblock_log_enhance_1_8_2"
      ],
      "description": "Modify the WorkflowDriver to retry failing plan steps up to a limit. If a step fails after exhausting retries, mark the task as 'Blocked' in ROADMAP.json with a reason and move to the next task."
    },
    {
      "task_id": "task_1_8_Z_implement_llm_rate_limiting",
      "priority": "Critical",
      "task_name": "Implement Client-Side Rate Limiting for Gemini API Calls",
      "status": "Completed",
      "target_file": "src/core/llm_orchestration.py",
      "depends_on": [
        "task_unblock_retry_limit_1_8_2"
      ],
      "description": "Modify LLMOrchestrator to add client-side rate limiting for Gemini API calls to prevent 429 errors. 1. Add attributes (`_last_gemini_call_start_time`, `_gemini_call_lock`) to LLMOrchestrator. 2. Implement a method `_apply_gemini_rate_limit` that calculates the necessary sleep duration based on a 10 RPM (6 seconds per request) limit, uses a lock for thread safety, applies `time.sleep` if needed, and updates `_gemini_call_start_time`. 3. Call this method in `_generate_with_retry` before invoking `_gemini_generate`. 4. Add unit tests for the rate limiting logic in `tests/test_llm_orchestration.py` to verify correct sleep timing under various conditions. 5. Ensure logging indicates when rate limiting is active."
    },
    {
      "task_id": "task_1_8_X_fix_multi_target_handling",
      "priority": "Critical",
      "task_name": "Correctly Handle Single Target File From Multi-Target Tasks in CodeGen",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_Z_implement_llm_rate_limiting"
      ],
      "description": "Modify `WorkflowDriver.autonomous_loop` to correctly process tasks with multiple comma-separated `target_file` entries. 1. When a code generation step is identified, and the parent task's `target_file` lists multiple files, implement logic to determine the *actual single target file* for the current plan step. This determination should prioritize explicit file mentions in the step description (e.g., 'modify fileA.py', 'in fileB.py'). If no specific file from the task's list is mentioned in the step, it should default to the first file in the `task_target_file` list and log a warning about the ambiguity. 2. Ensure the determined single file path is used for `_read_file_for_context`, pre-write validation, and `_write_output_file`. 3. Add unit tests for this parsing/determination logic in tests/test_phase1_8_features.py. 4. Add an integration test with a multi-target task and a step modifying one specific file, asserting correct file operations."
    },
    {
      "task_id": "task_1_8_Y_ensure_docstrings_in_codegen",
      "priority": "Critical",
      "task_name": "Ensure Docstrings in CoderLLM Output for Python Code",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_X_fix_multi_target_handling"
      ],
      "description": "Modify `WorkflowDriver.autonomous_loop` prompt construction for the CoderLLM. 1. When a plan step involves Python code generation that is likely to create new functions, methods, or classes, explicitly instruct the CoderLLM in its prompt to include comprehensive docstrings. The instruction should specify that docstrings must explain the purpose, arguments (name, type, description), and return values (type, description). Example instruction: 'IMPORTANT: For any new Python functions, methods, or classes, you MUST include a comprehensive PEP 257 compliant docstring. Use Google-style format (Args:, Returns:, Example: sections). This is required to pass automated ethical and style checks.'. 2. Add unit tests to verify that CoderLLM prompts for Python function/method generation include this instruction in tests/test_phase1_8_features.py."
    },
    {
      "task_id": "task_1_8_fix_rate_limit_enh_orchestrator",
      "priority": "Critical",
      "task_name": "Fix Gemini Rate Limiting in EnhancedLLMOrchestrator",
      "status": "Completed",
      "target_file": "src/core/llm_orchestration.py",
      "depends_on": [
        "task_1_8_Y_ensure_docstrings_in_codegen"
      ],
      "description": "The `_call_llm_api` method in `EnhancedLLMOrchestrator` directly calls `_gemini_generate` without going through `_generate_with_retry`, bypassing the rate limiting logic. Modify `_call_llm_api` to correctly incorporate rate limiting for Gemini calls by directly invoking `self._apply_gemini_rate_limit()` before `self._gemini_generate(text)` if the model is 'gemini'. Add unit tests to verify rate limiting is applied in this specific execution path."
    },
    {
      "task_id": "task_1_8_improve_snippet_handling",
      "priority": "Critical",
      "task_name": "Improve Code Snippet Generation, Validation, and Merging Robustness",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, src/core/llm_orchestration.py",
      "depends_on": [
        "task_1_8_fix_rate_limit_enh_orchestrator"
      ],
      "description": "The system frequently encounters syntax errors (e.g., 'unterminated string literal', 'unexpected indent') during pre-write validation (`ast.parse`) of CoderLLM-generated snippets or after merging. This blocks autonomous development. This task aims to improve robustness: 1. **Enhanced Logging:** (Completed) Modify `WorkflowDriver._invoke_coder_llm` to save the exact `generated_snippet` string (using `repr()`) to a temporary file if `ast.parse` fails, to capture hidden characters/malformations for debugging. 2. **Prompt Refinement:** (Completed) Review and refine CoderLLM prompts in `workflow_driver.py` to explicitly guide the LLM to output syntactically correct, complete, and context-aware code snippets, minimizing issues with string literals, indentation, and partial outputs. 3. **Merge Strategy Review:** (Completed) Modify the `_merge_snippet` method in `workflow_driver.py` to attempt basic indentation adjustment of the snippet if `METAMORPHIC_INSERT_POINT` is found. 4. **Pre-Merge Full File Syntax Check:** (Completed) Before `_merge_snippet` is called, create a temporary in-memory version of the target file content with the snippet hypothetically inserted at the `METAMORPHIC_INSERT_POINT`. Attempt `ast.parse()` on this *full temporary content*. If this full parse fails, the step should fail with specific feedback indicating an integration syntax error, rather than just the snippet failing."
    },
    {
      "task_id": "task_1_8_18_fix_string_literal_prompting",
      "priority": "Critical",
      "task_name": "Harden CoderLLM Prompting to Prevent String Literal Syntax Errors",
      "status": "Completed",
      "target_file": "src/core/constants.py",
      "depends_on": [
        "task_1_8_improve_snippet_handling"
      ],
      "description": "The CoderLLM is repeatedly generating snippets with `SyntaxError: unterminated triple-quoted string literal`. To fix this, modify the `GENERAL_SNIPPET_GUIDELINES` constant in `src/core/constants.py`. Add a new, high-priority rule at the top of the guidelines under a heading like `CRITICAL SYNTAX RULES`. This rule must explicitly instruct the LLM on how to correctly handle string literals to avoid this specific error. Key points to include in the instruction are: 1. All string literals MUST be correctly terminated with matching quotes. 2. Be extremely careful with raw strings (r'...'), as a backslash (`\\`) cannot be the final character. 3. When a string literal must contain code (like a test case or a prompt), ALWAYS use triple quotes (`\"\"\"` or `'''`). Provide clear examples of correct and incorrect usage."
    },
    {
      "task_id": "task_1_8_A_optimize_large_context_epic",
      "priority": "Critical",
      "task_name": "EPIC: Optimize Large Context Handling for Code Generation Steps",
      "status": "In Progress",
      "target_file": "src/core/automation/workflow_driver.py, src/core/llm_orchestration.py, src/core/chunking/recursive_summarizer.py",
      "depends_on": [
        "task_1_8_Y_ensure_docstrings_in_codegen"
      ],
      "description": "OVERARCHING GOAL: Refine `WorkflowDriver.autonomous_loop` and related components to significantly reduce redundant LLM calls when processing large context files for code generation steps, by providing minimal context for simple additions and prompting for only changed lines. This epic is achieved via sub-tasks task_1_8_A_1_helper_is_simple_addition through task_1_8_A_6."
    },
    {
      "task_id": "task_1_8_A_1a_create_skeleton",
      "priority": "Critical",
      "task_name": "Create Skeleton for _is_simple_addition_plan_step Helper",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_improve_snippet_handling"
      ],
      "description": "In `WorkflowDriver`, create the new helper method `_is_simple_addition_plan_step(self, plan_step_description: str) -> bool`. The implementation should only contain a `pass` statement. A comprehensive docstring explaining the method's purpose, arguments, and return value must be included. **Ensure the method signature and docstring adhere strictly to PEP8 line length (max 79 chars) and formatting guidelines.** This creates the necessary structure for the subsequent implementation task."
    },
    {
      "task_id": "task_1_8_A_1b_implement_logic",
      "priority": "Critical",
      "task_name": "Implement Logic for _is_simple_addition_plan_step",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_A_1a_create_skeleton"
      ],
      "description": "In `WorkflowDriver`, implement the logic for the `_is_simple_addition_plan_step` method. Use the `re` module to check the `plan_step_description` against a list of regex patterns that identify simple additions (e.g., adding an import, a new method, a constant). Ensure `import re` is present at the top of the file. The method should return True if a pattern matches, and False otherwise. **After implementation, ensure the code passes all Flake8 style and security checks (especially E501 line length) and that `import re` is correctly placed.**"
    },
    {
      "task_id": "task_1_8_A_1c_add_tests",
      "priority": "High",
      "task_name": "Add Unit Tests for _is_simple_addition_plan_step",
      "status": "Completed",
      "target_file": "tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_A_1b_implement_logic"
      ],
      "description": "In a new test file `tests/test_phase1_8_features.py`, add comprehensive unit tests for the `WorkflowDriver._is_simple_addition_plan_step` method. Use `pytest.mark.parametrize` to cover various cases: simple additions that should return True, complex modifications that should return False, and edge cases like empty strings. **Ensure tests are self-contained and pass `pytest` and `flake8` checks.**"
    },
    {
      "task_id": "task_1_8_A_2_implement_and_test_get_context_type",
      "priority": "Critical",
      "task_name": "Implement and Test _get_context_type_for_step Method",
      "description": "In `WorkflowDriver`, create and implement the logic for the `_get_context_type_for_step(self, step_description: str) -> Optional[str]` helper method. Use the `re` module to check the `step_description` against a list of regex patterns that identify context types (e.g., 'add_import', 'add_method_to_class'). The method should return the context type as a string if a pattern matches, and None otherwise. Also, add comprehensive unit tests for this new method in `tests/test_phase1_8_features.py` to ensure its correctness, including handling of null or empty inputs.",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_A_1c_add_tests"
      ]
    },
    {
      "task_id": "task_1_8_B_enhance_retry_prompts",
      "priority": "High",
      "task_name": "Enhance Retry Prompts with Specific Validation Feedback",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_A_optimize_large_context_epic"
      ],
      "description": "Modify the retry logic within `WorkflowDriver.autonomous_loop`. When a plan step fails due to pre-write validation (syntax, style, ethics) or other captured errors (e.g., test failures post-write), the specific error message(s) should be extracted from `self._current_task_results['pre_write_validation_feedback']` or `self._current_task_results['step_errors']`. This detailed feedback must be appended to the prompt for the CoderLLM during the subsequent retry attempt. The goal is to provide the LLM with concrete information about what went wrong, enabling it to self-correct more effectively."
    },
    {
      "task_id": "task_1_8_2_A_add_max_retries_constant",
      "priority": "Critical",
      "task_name": "Add MAX_STEP_RETRIES Constant",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_B_enhance_retry_prompts"
      ],
      "description": "In `WorkflowDriver`, add a class-level constant `MAX_STEP_RETRIES` and set its value to 2. This constant will define the number of retries for a failing plan step.",
      "success_criteria": "The constant `MAX_STEP_RETRIES = 2` is correctly added to the `WorkflowDriver` class. The file compiles and all existing tests pass."
    },
    {
      "task_id": "task_1_8_2_B_implement_while_loop",
      "priority": "Critical",
      "task_name": "Implement While Loop for Step Retries",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_A_add_max_retries_constant"
      ],
      "description": "In `WorkflowDriver.autonomous_loop`, wrap the entire `try...except` block for a single step's execution (from `logger.info(f\"Executing step...`) inside a `while step_retries <= MAX_STEP_RETRIES:` loop. This will enable re-execution of a failing step.",
      "success_criteria": "The step execution `try...except` block is correctly enclosed within a `while` loop that uses the `MAX_STEP_RETRIES` constant. The file compiles and all existing tests must pass."
    },
    {
      "task_id": "task_1_8_2_C_add_retry_counter",
      "priority": "Critical",
      "task_name": "Add and Increment Step Retry Counter",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_B_implement_while_loop"
      ],
      "description": "In `WorkflowDriver.autonomous_loop`, initialize a `step_retries` counter to 0 before the `while` loop for step execution. Inside the `except` block of the step execution, increment this counter (`step_retries += 1`).",
      "success_criteria": "The `step_retries` counter is initialized before the loop and incremented within the `except` block. The file must compile and all existing tests pass."
    },
    {
      "task_id": "task_1_8_2_D_add_feedback_reinvocation",
      "priority": "Critical",
      "task_name": "Add Feedback Construction for Retry Prompts",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_C_add_retry_counter"
      ],
      "description": "In `WorkflowDriver.autonomous_loop`, inside the `except` block for step execution, capture the exception details. Construct a `retry_feedback_for_llm_prompt` string from the error. This feedback string should then be passed to the `_construct_coder_llm_prompt` method on the next iteration of the `while` loop to guide the Coder LLM.",
      "success_criteria": "A feedback string is constructed from the exception and passed to the Coder LLM prompt on retries. The file must compile and all existing tests pass."
    },
    {
      "task_id": "task_1_8_2_E_add_exhaustion_error",
      "priority": "Critical",
      "task_name": "Add Logic to Block Task on Retry Exhaustion",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_D_add_feedback_reinvocation"
      ],
      "description": "In `WorkflowDriver.autonomous_loop`, after the `step_retries` counter is incremented, add a check: `if step_retries > MAX_STEP_RETRIES:`. Inside this block, log a final error message, mark the task as failed (`task_failed_step = True`), and `break` from the `while` loop. This will cause the task to be marked as 'Blocked'.",
      "success_criteria": "Logic to check for exhausted retries, log an error, and break the loop is correctly implemented. The file must compile and all existing tests pass."
    },
    {
      "task_id": "task_1_8_2_F_1_add_constant",
      "priority": "High",
      "task_name": "Add CONTEXT_LEAKAGE_INDICATORS Constant",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_E_add_exhaustion_error"
      ],
      "description": "In `src/core/automation/workflow_driver.py`, add a class-level constant `CONTEXT_LEAKAGE_INDICATORS: List[str]` to the `WorkflowDriver` class. Initialize it with `['```python', 'As an AI language model', 'I am a large...`."
    },
    {
      "task_id": "task_1_8_2_F_2_add_method",
      "priority": "High",
      "task_name": "Implement _validate_for_context_leakage Method",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_F_1_add_constant"
      ],
      "description": "In `src/core/automation/workflow_driver.py`, add a new private method `_validate_for_context_leakage(self, code_snippet: str) -> bool` to the `WorkflowDriver` class. This method should iterate through the `CONTEXT_LEAKAGE_INDICATORS` constant and return `False` if any indicator string is found within the `code_snippet`. If no indicators are found, it should return `True`."
    },
    {
      "task_id": "task_1_8_2_F_3_integrate_call",
      "priority": "High",
      "task_name": "Integrate Context Leakage Check into Pre-Write Validation",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_F_2_add_method"
      ],
      "description": "In `src/core/automation/workflow_driver.py`, integrate the new `_validate_for_context_leakage` method into the pre-write validation sequence within the `autonomous_loop`. Ensure that if the method returns `False`, a descriptive error message is added to the `validation_feedback` list, causing the step to fail and retry."
    },
    {
      "task_id": "task_1_8_2_F_4a_create_test_class",
      "priority": "High",
      "task_name": "Create Unittest-based Test Class for Context Leakage Validation",
      "status": "Completed",
      "target_file": "tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_2_F_3_integrate_call"
      ],
      "description": "In `tests/test_phase1_8_features.py`, add `import unittest` at the top of the file, and then create a new empty `unittest.TestCase` class named `TestContextLeakageValidationUnittest`. The class should have a docstring explaining its purpose and a `pass` statement as its body. **This must be done in a single code modification step** to ensure the `import unittest` and its usage are added together, preventing 'unused import' errors."
    },
    {
      "task_id": "task_1_8_2_F_4b_define_leakage_snippets",
      "priority": "High",
      "task_name": "Define Context Leakage Snippets List",
      "status": "Completed",
      "target_file": "tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_2_F_4a_create_test_class"
      ],
      "description": "In `tests/test_phase1_8_features.py`, inside the `TestContextLeakageValidationUnittest` class, define a class-level variable named `LEAKAGE_SNIPPETS`. This variable must be a Python list of strings. Each string should be a simple example of context leakage. Use raw, triple-quoted strings for each entry (e.g., `r\"\"\"As an AI, I think...\"\"\"`). Do not include complex examples with nested triple quotes for this step."
    },
    {
      "task_id": "task_1_8_2_F_4b_create_failing_test_method",
      "priority": "High",
      "task_name": "Create Failing Test Method for Context Leakage",
      "status": "Completed",
      "target_file": "tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_2_F_4b_define_leakage_snippets"
      ],
      "description": "In `tests/test_phase1_8_features.py`, add a new test method `test_context_leakage_detection_finds_leaks` to the `TestContextLeakageValidationUnittest` class. This method must be decorated with `@pytest.mark.parametrize('snippet', TestContextLeakageValidationUnittest.LEAKAGE_SNIPPETS)`. Inside the test method, call `WorkflowDriver._validate_for_context_leakage(snippet)` and assert that the result is `False`. This test is expected to fail until the validation logic is implemented correctly."
    },
    {
      "task_id": "task_1_8_2_F_4c_syntax_fix",
      "priority": "Critical",
      "task_name": "Fix Pre-existing Syntax Error in test_phase1_8_features.py",
      "status": "Completed",
      "target_file": "tests/test_phase1_8_features.py",
      "depends_on": [],
      "description": "The file `tests/test_phase1_8_features.py` contains a `SyntaxError: unterminated string literal` on line 262 (or nearby), specifically `mock_context_for_llm = \"class WorkflowDriver:\\n`. This string literal needs to be corrected to use triple quotes (`\"\"\"` or `'''`) to properly span multiple lines and be correctly terminated. This task is critical to unblock further development on `task_1_8_2_F_4c_add_passing_tests` and subsequent tasks."
    },
    {
      "task_id": "task_1_8_2_F_4c_add_passing_tests",
      "priority": "High",
      "task_name": "Add Passing Unit Tests for Compliant Snippets",
      "status": "Not Started",
      "target_file": "tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_2_F_4b_create_failing_test_method",
        "task_1_8_2_F_4c_syntax_fix",
        "task_1_8_19b_add_tests_for_differentiation"
      ],
      "description": "In `tests/test_phase1_8_features.py`, add another new test method to the `TestContextLeakageValidationUnittest` class. This method MUST use `pytest.mark.parametrize` to test multiple code snippets that *do not* contain any context leakage indicators. For each compliant snippet, the test must assert that `WorkflowDriver._validate_for_context_leakage` correctly returns `True`. Ensure all test case strings are defined using triple quotes (`\"\"\"` or `'''`)."
    },
    {
      "task_id": "task_1_8_2_G_atomic_planner",
      "priority": "Critical",
      "task_name": "Implement Atomic Step Recognition in Planner",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_F_4c_add_passing_tests"
      ],
      "description": "Enhance the planning agent to recognize when a task description requires multiple code changes to be performed in a single, atomic step (e.g., using keywords like 'in a single step', 'atomically', 'together'). When detected, the planner should generate a single, combined plan step for the Coder LLM, rather than multiple separate code generation steps. This prevents intermediate validation failures (e.g., 'F401: imported but unused' as seen in task_1_8_2_F_4a_create_test_class) and ensures logical changes are applied together."
    },
    {
      "task_id": "task_1_8_2_H_lint_codebase",
      "priority": "Critical",
      "task_name": "Codebase Linting and Style Conformance",
      "status": "Not Started",
      "target_file": "src/, tests/",
      "depends_on": [
        "task_1_8_2_G_atomic_planner"
      ],
      "description": "Run a linter (e.g., `flake8`) and an autoformatter (e.g., `black`) across the entire `src` and `tests` directories to fix existing style violations (PEP8, etc.). This will reduce noise in future validation steps, allowing the autonomous system to more accurately identify new issues introduced by its own code modifications. This task should be completed before proceeding with further feature development."
    },
    {
      "task_id": "task_1_8_19a_1_wrap_ast_parse",
      "priority": "High",
      "task_name": "Wrap ast.parse in a try-except block for SyntaxError",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_18_fix_string_literal_prompting"
      ],
      "description": "In `_execute_code_generation_step`, find the `ast.parse(content_for_ast_check)` call. Wrap this specific call in a `try...except SyntaxError as se:` block. For now, inside the `except` block, simply re-raise the exception using `raise se`. This is the first step to build the differentiation logic, enabling safe detection of syntax errors without crashing the workflow."
    },
    {
      "task_id": "task_1_8_19a_2_get_line_ranges",
      "priority": "High",
      "task_name": "Get and Log Snippet and Error Line Numbers",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_19a_1_wrap_ast_parse"
      ],
      "description": "Inside the `except SyntaxError as se:` block (before the `raise`), get the error line number using `se.lineno`. Also, call `self._get_hypothetical_snippet_line_range(original_full_content, cleaned_output)` to get the snippet's line range. Log these values for debugging purposes, providing crucial context for error differentiation."
    },
    {
      "task_id": "task_1_8_19a_3_implement_conditional_logic",
      "priority": "High",
      "task_name": "Implement Conditional Logic for Error Location",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_19a_2_get_line_ranges"
      ],
      "description": "Inside the `except` block, add an `if` statement to check if `se.lineno` is *not* within the range returned by `_get_hypothetical_snippet_line_range`. This implements the core logic for differentiating pre-existing errors from snippet-introduced ones. For now, leave the `if` and `else` blocks empty (`pass`). Remove the `raise se` statement."
    },
    {
      "task_id": "task_1_8_19a_4_handle_preexisting_error",
      "priority": "High",
      "task_name": "Implement Handling for Pre-existing Source File Errors",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_19a_3_implement_conditional_logic"
      ],
      "description": "In the `if` block (for errors outside the snippet), log an error message indicating a pre-existing syntax error was found. Then, raise a `ValueError` with a descriptive message to fail the step. This will trigger the task blocking mechanism, preventing the system from attempting to fix errors it didn't introduce."
    },
    {
      "task_id": "task_1_8_19a_5_handle_snippet_error",
      "priority": "High",
      "task_name": "Implement Handling for Snippet-Introduced Errors",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_19a_4_handle_preexisting_error"
      ],
      "description": "In the `else` block (for errors inside the snippet), re-introduce the original failure logic. This involves setting `validation_passed = False`, creating an error message string, and appending it to `validation_feedback`. This ensures that errors caused by the LLM's generated code are correctly flagged for remediation."
    },
    {
      "task_id": "task_1_8_19b_add_tests_for_differentiation",
      "priority": "High",
      "task_name": "Add Unit Tests for Syntax Error Differentiation Logic",
      "status": "Not Started",
      "target_file": "tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_19a_5_handle_snippet_error"
      ],
      "description": "In `tests/test_phase1_8_features.py`, add new unit tests to verify the syntax error differentiation logic implemented in `task_1_8_19a`. The tests should cover cases where a `SyntaxError` occurs within the generated snippet and cases where the error exists in the original source file content, outside the snippet's range. Use `pytest.raises` and mock `ast.parse` to simulate these `SyntaxError` conditions."
    },
    {
      "task_id": "task_1_8_20_scoped_validation",
      "priority": "High",
      "task_name": "Implement Scoped Validation for Code Review and Ethics",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py,src/core/agents/code_review_agent.py",
      "depends_on": [
        "task_1_8_2_H_lint_codebase"
      ],
      "description": "Modify the validation logic in the WorkflowDriver to perform 'scoped' analysis. Instead of validating the entire file after a change, generate a diff of the proposed change and run linters (Flake8) and ethical checks only on the added/modified lines. This prevents the system from being blocked by pre-existing issues in a file and provides more targeted feedback to the Coder LLM."
    },
    {
      "task_id": "task_1_8_C_1_analyze_A1_failure_and_improve_codegen",
      "priority": "High",
      "task_name": "Analyze task_1_8_A_1 Failures & Improve CodeGen",
      "status": "Not Started",
      "target_file": "src/core/llm/orchestration.py",
      "depends_on": [
        "task_1_8_A_optimize_large_context_epic"
      ],
      "description": "Analyze the failures encountered during the initial autonomous attempt of task_1_8_A_1 (_is_simple_addition_plan_step). Specifically, investigate why the CoderLLM failed to: 1. Correctly include `import re`. 2. Adhere to style guidelines (E302, E501). 3. Consistently include docstrings for new methods. 4. Avoided outputting spurious code that was appended to files. Implement improvements to CoderLLM prompting, snippet cleaning, or pre-write validation to prevent such issues in the future. This task focuses on the AI improving its own code generation process based on past failures."
    },
    {
      "task_id": "task_1_8_C_2_focus_instruction",
      "priority": "Critical",
      "task_name": "Add Focus Instruction to CoderLLM Prompt for New Code Blocks",
      "status": "Not Started",
      "target_file": "src/core/constants.py, src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_F_4c_add_passing_tests"
      ],
      "description": "To prevent the CoderLLM from getting distracted by large context files when generating new, self-contained code blocks (like methods or classes), update the prompt construction logic. Modify the `CRITICAL_CODER_LLM_FULL_BLOCK_OUTPUT_INSTRUCTIONS` constant in `src/core/constants.py` to include a strong instruction for the LLM to focus *only* on the specific plan step, not the overall task description. Then, update `_construct_coder_llm_prompt` in `workflow_driver.py` to conditionally include this new instruction when generating full blocks."
    },
    {
      "task_id": "task_1_8_3",
      "priority": "Critical",
      "task_name": "Implement Step-Level Remediation Loop",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_F_3_integrate_call"
      ],
      "description": "If pre-write validation fails, provide targeted feedback to the Coder LLM and retry generating the snippet for that specific step (e.g., 2-3 attempts) before failing the step or task."
    },
    {
      "task_id": "task_1_8_4",
      "priority": "Critical",
      "task_name": "Ensure Post-Write, Step-Level Test Execution",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_3"
      ],
      "description": "Modify the Workflow Driver to automatically trigger relevant tests (using execute_tests) after any plan step that successfully modifies code, especially if the task has a target_file that implies testability (e.g., .py file). Capture results in _current_task_results."
    },
    {
      "task_id": "task_1_8_5",
      "priority": "High",
      "task_name": "Implement Learning from Failures (Data Capture)",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_4"
      ],
      "description": "Log detailed information about validation failures, remediation attempts, LLM prompts/responses, and outcomes. Store this structured data in the Knowledge Graph or a dedicated database for analysis and learning."
    },
    {
      "task_id": "task_1_8_14",
      "priority": "High",
      "task_name": "Address Ethical Debt in Token Allocator Policy",
      "status": "Not Started",
      "target_file": "src/core/ethics/constraints.py, src/core/optimization/adaptive_token_allocator.py",
      "depends_on": [
        "task_1_8_A_optimize_large_context_epic",
        "task_1_8_5"
      ],
      "description": "The temporary fix for AllocationError in EthicalAllocationPolicy bypassed diversity constraints. Implement a robust, ethical token allocation strategy that correctly handles large prompts and multiple chunks, potentially revising Z3 constraints or exploring alternative allocation methods. Ensure the policy promotes model diversity where feasible without causing allocation failures."
    },
    {
      "task_id": "task_1_8_7",
      "priority": "Medium",
      "task_name": "Implement Automated Task Decomposition",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_1",
        "task_1_8_5"
      ],
      "description": "Add logic to the Workflow Driver or a Planning Agent to assess task complexity and automatically break down large tasks into smaller, dependent sub-tasks in the roadmap if needed. As a reference implementation, analyze the manual decomposition of `task_1_8_19a` into `task_1_8_19a_1` through `task_1_8_19a_5`, which breaks down a complex code modification into atomic, verifiable steps. This decomposition reduces LLM cognitive load and improves retry effectiveness."
    },
    {
      "task_id": "task_1_8_8",
      "priority": "Medium",
      "task_name": "Refine Grade Report & Error Logging",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_5"
      ],
      "description": "Update generate_grade_report to clearly distinguish validation execution errors from findings and highlight critical issues. Improve overall logging clarity for debugging autonomous runs."
    },
    {
      "task_id": "task_1_8_9",
      "priority": "High",
      "task_name": "Implement Advanced Code Merging",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_3"
      ],
      "description": "Replace the current string-based _merge_snippet with an AST-aware merging utility to handle code modifications more robustly and reduce the chance of introducing syntax errors during merging."
    },
    {
      "task_id": "task_1_8_10",
      "priority": "High",
      "task_name": "Implement Prompt Self-Correction Mechanism",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_3",
        "task_1_8_6"
      ],
      "description": "If remediation attempts fail for a step, analyze the failure and modify the *prompt* sent to the Coder LLM for the next attempt, or potentially modify the original code generation prompt template based on failure patterns."
    },
    {
      "task_id": "task_1_8_11_auto_refinement",
      "priority": "High",
      "task_name": "Implement Self-Healing Task Descriptions via Refinement Agent",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_10"
      ],
      "description": "Implement a mechanism for the system to self-correct by refining its own roadmap. When a task is marked 'Blocked' due to repeated step failures (e.g., `SyntaxError` during code generation), a new 'Refinement Agent' should be invoked. This agent will analyze the blocked task's description, the failing step, and the final error message (reason_blocked). It will then generate a new, more detailed and prescriptive description for the task and patch the ROADMAP.json file. This allows the system to learn from its failures at a strategic level, not just a code-generation level. The agent should leverage error-specific templates (e.g., for syntax errors, add explicit formatting rules to the description like preferring raw triple-quoted strings). **The Refinement Agent should be triggered after a task has failed 3 consecutive times due to code generation issues.**"
    },
    {
      "task_id": "task_1_8_11",
      "priority": "High",
      "task_name": "Improve Coder LLM Prompt Generation Logic",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_5",
        "task_1_8_10"
      ],
      "description": "Refine how the Workflow Driver constructs prompts for the Coder LLM, incorporating lessons learned from failure data (task_1_8_5) and successful prompt self-correction (task_1_8_10)."
    },
    {
      "task_id": "task_1_8_12",
      "priority": "Medium",
      "task_name": "Implement Task Success Prediction",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_5"
      ],
      "description": "Develop a simple model (potentially using data from task_1_8_5) to predict the likelihood of autonomous success for a given task based on its characteristics. Use this to inform task selection or flag low-probability tasks for manual review."
    },
    {
      "task_id": "task_1_8_13",
      "priority": "High",
      "task_name": "Add Comprehensive Tests for Phase 1.8 Features",
      "status": "Not Started",
      "target_file": "tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_1",
        "task_1_8_2_F_3_integrate_call",
        "task_1_8_3",
        "task_1_8_4",
        "task_1_8_5",
        "task_1_8_7",
        "task_1_8_8",
        "task_1_8_9",
        "task_1_8_10",
        "task_1_8_11",
        "task_1_8_12"
      ],
      "description": "Write comprehensive unit/integration tests covering all new features in Phase 1.8, including pre-write validation, step-level retries, post-write test execution, improved remediation logic, failure data capture, task decomposition, advanced merging, prompt self-correction, improved prompt generation, and task success prediction."
    },
    {
      "task_id": "task_1_8_15_plan_complexity_detection",
      "priority": "High",
      "task_name": "Implement Plan Complexity Detection and Re-Planning",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_7",
        "task_1_8_5"
      ],
      "description": "Develop and integrate a mechanism within the WorkflowDriver to analyze the complexity of generated solution plans. If a plan for a simple task (e.g., short description, few action verbs) is excessively long or contains too many granular steps, the system should flag it and trigger a re-planning attempt with a refined prompt, guiding the planner towards simpler, more atomic plans.",
      "success_criteria": "The system can accurately identify overly complex plans for simple tasks and successfully trigger re-planning, resulting in more concise and effective plans."
    },
    {
      "task_id": "task_1_8_16_context_scoping_refinement",
      "priority": "Not Started",
      "task_name": "Refine Coder LLM Context Scoping for Atomic Modifications",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_A_optimize_large_context_epic",
        "task_1_8_5"
      ],
      "description": "Enhance the `WorkflowDriver`'s `_construct_coder_llm_prompt` and `_extract_targeted_context` methods. For plan steps identified as very small, atomic code modifications (e.g., inserting a single line, changing a variable initialization), the system should provide the Coder LLM with a highly localized context (e.g., 5-10 lines around the insertion/modification point) instead of the entire file. This aims to reduce LLM hallucination and improve the precision of code generation for minor changes.",
      "success_criteria": "The Coder LLM receives appropriately scoped context for atomic modifications, leading to fewer hallucinations and higher success rates for small code changes. Unit tests verify correct context extraction for various atomic modification scenarios."
    },
    {
      "task_id": "task_1_8_17_robust_step_classification",
      "priority": "Not Started",
      "task_name": "Implement Robust Plan Step Classification (AST/Semantic)",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_A_2_implement_and_test_get_context_type",
        "task_1_8_5"
      ],
      "description": "Replace or significantly enhance the current regex-based plan step classification in `WorkflowDriver` with a more robust, semantic approach (e.g., AST parsing of step descriptions, NLP semantic understanding, or a fine-tuned model). This aims to accurately identify the intent of each step (e.g., 'add constant', 'modify method', 'refactor block') to ensure precise context is provided to the Coder LLM, reducing hallucinations and improving code generation accuracy."
    },
    {
      "task_id": "task_1_8_18_pre_write_diff_validation",
      "priority": "Not Started",
      "task_name": "Implement Pre-Write Diff Validation Guardrail",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_improve_snippet_handling",
        "task_1_8_5"
      ],
      "description": "Introduce a new pre-write validation step in `WorkflowDriver.autonomous_loop`. Before writing the merged code, generate a diff between the original file content and the proposed merged content. If the diff indicates unintended modifications (e.g., changes outside the targeted area for a 'simple addition' task, or excessive unrelated lines), reject the change and provide specific feedback to the Coder LLM for remediation. This acts as a crucial guardrail against LLM over-generation or misinterpretation."
    },
    {
      "task_id": "task_1_8_11b_failure_driven_decomposition",
      "priority": "High",
      "task_name": "Implement Failure-Driven Task Decomposition",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_11_auto_refinement"
      ],
      "description": "Enhance the Refinement Agent (from task_1_8_11_auto_refinement). When a task is blocked because a plan step failed all retries, and that step is not the last in the plan, the agent should attempt to decompose the task. It will split the original task into two: one containing the successful steps up to (but not including) the failure point, which will be marked as 'Completed'. The second, new task will be a copy of the original but with its plan starting from the failing step, and it will depend on the first part. This allows the system to make incremental progress on complex tasks that fail midway through their plan."
    },
    {
      "task_id": "task_1_8_21_raw_string_validation",
      "priority": "High",
      "task_name": "Implement Targeted Raw String Literal Validation",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_improve_snippet_handling"
      ],
      "description": "Enhance pre-write validation in `WorkflowDriver` with a targeted regex check to specifically detect malformed raw string literals (e.g., `r\"\"\"`) in generated Python code. This acts as an immediate guardrail to catch common LLM syntax errors related to string escaping before `ast.parse`. Example regex check: `if re.search(r'\\br[\"\\'][\"\\']{2}', code_snippet): raise ValueError('Invalid raw string')`."
    },
    {
      "task_id": "task_1_8_22_roadmap_example_style_guide",
      "priority": "Medium",
      "task_name": "Update CONTRIBUTING.md for ROADMAP.json Example Style",
      "status": "Not Started",
      "target_file": "CONTRIBUTING.md",
      "depends_on": [],
      "description": "Update the `CONTRIBUTING.md` file to include a style guide rule for providing Python code examples within `ROADMAP.json` descriptions. The rule should recommend preferring raw triple-single-quoted strings (`r'''...'''`) or raw triple-double-quoted strings (`r\"\"\"...\"\"\"`) for multi-line code blocks, or careful escaping with single-quoted JSON strings (`'...'`) to avoid ambiguity and `SyntaxError` for LLMs. Example: `\u2705 r'''class Example:\\n    pass'''` vs. `\u274c \"class Example:\\n    pass\"`."
    },
    {
      "task_id": "task_1_8_23_failing_test_success_criteria",
      "priority": "High",
      "task_name": "Implement Success Criteria for Intentionally Failing Tests",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_4"
      ],
      "description": "Enhance the `_parse_and_evaluate_grade_report` method to handle tasks that are expected to produce failing tests. Add a field to the task definition in `ROADMAP.json` (e.g., `\"expected_outcome\": \"tests_fail\"`). If this field is present, the grading logic should treat a 'failed' test status as a success for that specific task, allowing the system to proceed with Test-Driven Development workflows autonomously."
    }
  ],
  "next_phase_actions": [
    "Set `status`: `Completed` on all Phase 1.8 tasks.",
    "Update the `phase`, `phase_goal`, and `current_focus` fields to 'Phase 2 Iteration 2: Enhanced Agents & Knowledge Graph'."
  ],
  "current_focus": "🎯 CURRENT FOCUS: Phase 1.8 - Hardened Autonomous Loop & Advanced Remediation 🛠️"
}