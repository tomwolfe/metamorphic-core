{
  "phase": "Phase 1.8: Hardened Autonomous Loop & Advanced Remediation",
  "phase_goal": "Significantly improve the robustness and self-correction capabilities of the autonomous workflow loop based on real-world failure analysis, incorporating learning from failures, more sophisticated remediation, and better error handling.",
  "success_metrics": [
    "The Workflow Driver successfully handles and remediates common syntax, style, ethical, and test failures autonomously.",
    "Critical errors (like IndentationError) are caught and addressed at the step level before writing.",
    "Automated tests are executed after code modification steps.",
    "The remediation loop successfully fixes >= 85% of tasks that fail initial validation (excluding fundamental design/ambiguity issues).",
    "The Grade Report clearly distinguishes validation execution errors from findings and highlights critical issues.",
    "Detailed failure data is logged and stored for learning and analysis.",
    "The system can attempt to decompose complex tasks into smaller sub-tasks.",
    "Code merging is more robust and less likely to introduce syntax errors.",
    "The system can predict the likelihood of autonomous success for a task.",
    "Phase 1.8 tests achieve >= 95% code coverage for new logic."
  ],
  "tasks": [
    {
      "task_id": "task_1_8_1_pre_fix",
      "priority": "Critical",
      "task_name": "Refine Step Classification to Correctly Identify Research Steps",
      "description": "Modify WorkflowDriver's step classification logic to ensure that plan steps primarily involving 'Research and identify', 'Analyze', or 'Investigate' are not classified as code generation steps, even if they mention target files or code-related keywords as examples. This is a prerequisite to allow the main task_1_8_1 to proceed autonomously.",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": []
    },
    {
      "task_id": "task_1_8_0_fix_summarizer_synthesize",
      "priority": "Critical",
      "task_name": "Fix AttributeError in RecursiveSummarizer for synthesize method",
      "description": "The EnhancedLLMOrchestrator calls self.summarizer.synthesize(), but RecursiveSummarizer does not have this method. Add a synthesize(self, summaries: List[str]) -> str method to RecursiveSummarizer in src/core/chunking/recursive_summarizer.py that joins the list of summaries into a single string. This is needed to unblock code generation in _handle_large_context.",
      "status": "Completed",
      "target_file": "src/core/chunking/recursive_summarizer.py",
      "depends_on": [
        "task_1_8_1_pre_fix"
      ]
    },
    {
      "task_id": "task_1_8_0_fix_token_allocator",
      "priority": "Critical",
      "task_name": "Adjust TokenAllocator Cost Function for Realistic Allocations",
      "description": "The TokenAllocator's cost function _model_cost in src/core/optimization/adaptive_token_allocator.py has a quadratic term that heavily penalizes token count, resulting in minimal allocations (e.g., 101 tokens). Modify the coefficient of the quadratic term (e.g., change the divisor from 1000.0 to 10000000.0) to allow for more realistic token allocations for code generation tasks. Also, ensure the solver is reset before each allocation call.",
      "status": "Completed",
      "target_file": "src/core/optimization/adaptive_token_allocator.py, src/core/ethics/constraints.py",
      "depends_on": [
        "task_1_8_0_fix_summarizer_synthesize"
      ]
    },
    {
      "task_id": "task_1_8_1_unblock_overwrite_fix",
      "priority": "Critical",
      "task_name": "Prevent Placeholder Overwrite of Core Python Files for Conceptual Steps",
      "description": "Modify the WorkflowDriver's autonomous_loop logic. Specifically, in the 'explicit file writing' branch, add a condition to prevent writing placeholder content to the main Python task_target_file of a task if the plan step is conceptual (e.g., 'define a list', 'analyze requirements') and not an explicit 'create file' or 'generate file' instruction. Also, ensure Python-specific placeholders (#) are used if a placeholder write to a .py file is genuinely intended.",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_1_pre_fix",
        "task_1_8_0_fix_summarizer_synthesize",
        "task_1_8_0_fix_token_allocator"
      ]
    },
    {
      "task_id": "task_1_8_1_fix_syntax_and_add_tests",
      "priority": "Critical",
      "task_name": "Fix Syntax Error in classify_plan_step and Add Unit Tests",
      "description": "The code generated for task_1_8_1 (Enhance Plan Step Identification) included an erroneous trailing line causing a syntax/name error. Remove this line from src/core/automation/workflow_driver.py. Additionally, create comprehensive unit tests for the new classify_plan_step function. Tests should cover code steps, conceptual steps, ambiguous steps, and edge cases. Place tests in tests/test_phase1_8_features.py.",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_1_unblock_overwrite_fix"
      ]
    },
    {
      "task_id": "task_1_8_1",
      "priority": "Critical",
      "task_name": "Enhance Plan Step Identification",
      "description": "Improve the Workflow Driver's logic to better identify plan steps requiring code modification (imports, constants, definitions, etc.) vs. conceptual steps. Use more sophisticated keyword matching or NLP.",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_1_fix_syntax_and_add_tests"
      ]
    },
    {
      "task_id": "task_1_8_1b_increase_min_token_alloc",
      "priority": "Critical",
      "task_name": "Increase Minimum Token Allocation per Chunk",
      "description": "Modify TokenAllocator and EthicalAllocationPolicy to enforce a higher minimum token count per chunk (e.g., 1000 tokens instead of 100). This is crucial for effective code generation, as the current minimum leads to insufficient token allocation and stalls progress on code-generating tasks. This task addresses the shortcomings of task_1_8_0_fix_token_allocator.",
      "status": "Completed",
      "target_file": "src/core/optimization/adaptive_token_allocator.py, src/core/ethics/constraints.py",
      "depends_on": [
        "task_1_8_1"
      ]
    },
    {
      "task_id": "task_1_8_2b_fix_placeholder_overwrite_for_modification_steps",
      "priority": "Critical",
      "task_name": "Refine Placeholder Write Logic for Main Target Modification",
      "description": "Modify WorkflowDriver.autonomous_loop to prevent placeholder overwrites on the main Python task_target_file for steps that imply modification rather than creation (e.g., \"insert a block\", \"add logic to method X\"). Such steps, if classified as 'explicit file writing', should be treated as code generation steps or have their placeholder write skipped entirely if they are too vague for direct code generation. This refines task_1_8_1_unblock_overwrite_fix. Add unit tests for this specific logic in tests/test_phase1_8_features.py, covering scenarios where placeholders should and should not be written to the main Python target.",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": []
    },
    {
      "task_id": "task_1_8_2c_target_test_file_for_test_writing_steps",
      "priority": "Critical",
      "task_name": "Correctly Target Test Files for Unit Test Generation Steps",
      "description": "Enhance WorkflowDriver.autonomous_loop step processing. If a plan step is for writing unit tests (e.g., contains \"write unit tests for X\", mentions tests/test_*.py, or filepath_from_step points to a test file path), ensure the generated test code is written to the appropriate test file path. This path should be derived from filepath_from_step if valid and test-like, or by convention from task_target_file (e.g., src/A.py -> tests/test_A.py). This determined test file path should override task_target_file for the write operation if task_target_file is not itself a test file. Add unit tests for this file targeting logic in tests/test_phase1_8_features.py.",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_2b_fix_placeholder_overwrite_for_modification_steps"
      ]
    },
    {
      "task_id": "task_unblock_log_enhance_1_8_2",
      "priority": "Critical",
      "task_name": "Enhance Logging for Code Generation and Pre-Write Validation",
      "description": "Modify the WorkflowDriver to provide more detailed logging during code generation and pre-write validation. Log failed snippets, LLM prompts, and optimizer input/output (if accessible).",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2c_target_test_file_for_test_writing_steps"
      ]
    },
    {
      "task_id": "task_unblock_retry_limit_1_8_2",
      "priority": "Critical",
      "task_name": "Implement Step-Level Retry Limit and Task Blocking in WorkflowDriver",
      "description": "Modify the WorkflowDriver to retry failing plan steps up to a limit. If a step fails after exhausting retries, mark the task as 'Blocked' in ROADMAP.json with a reason and move to the next task.",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_unblock_log_enhance_1_8_2"
      ]
    },
    {
      "task_id": "task_1_8_Z_implement_llm_rate_limiting",
      "priority": "Critical",
      "task_name": "Implement Client-Side Rate Limiting for Gemini API Calls",
      "description": "Modify LLMOrchestrator to add client-side rate limiting for Gemini API calls to prevent 429 errors. 1. Add attributes (`_last_gemini_call_start_time`, `_gemini_call_lock`) to LLMOrchestrator. 2. Implement a method `_apply_gemini_rate_limit` that calculates the necessary sleep duration based on a 10 RPM (6 seconds per request) limit, uses a lock for thread safety, applies `time.sleep` if needed, and updates `_gemini_call_start_time`. 3. Call this method in `_generate_with_retry` before invoking `_gemini_generate`. 4. Add unit tests for the rate limiting logic in `tests/test_llm_orchestration.py` to verify correct sleep timing under various conditions. 5. Ensure logging indicates when rate limiting is active.",
      "status": "Completed",
      "target_file": "src/core/llm_orchestration.py",
      "depends_on": [
        "task_unblock_retry_limit_1_8_2"
      ]
    },
    {
      "task_id": "task_1_8_X_fix_multi_target_handling",
      "priority": "Critical",
      "task_name": "Correctly Handle Single Target File From Multi-Target Tasks in CodeGen",
      "description": "Modify `WorkflowDriver.autonomous_loop` to correctly process tasks with multiple comma-separated `target_file` entries. 1. When a code generation step is identified, and the parent task's `target_file` lists multiple files, implement logic to determine the *actual single target file* for the current plan step. This determination should prioritize explicit file mentions in the step description (e.g., 'modify fileA.py', 'in fileB.py'). If no specific file from the task's list is mentioned in the step, it should default to the first file in the `task_target_file` list and log a warning about the ambiguity. 2. Ensure the determined single file path is used for `_read_file_for_context`, pre-write validation, and `_write_output_file`. 3. Add unit tests for this parsing/determination logic in tests/test_phase1_8_features.py. 4. Add an integration test with a multi-target task and a step modifying one specific file, asserting correct file operations.",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_Z_implement_llm_rate_limiting"
      ]
    },
    {
      "task_id": "task_1_8_Y_ensure_docstrings_in_codegen",
      "priority": "Critical",
      "task_name": "Ensure Docstrings in CoderLLM Output for Python Code",
      "description": "Modify `WorkflowDriver.autonomous_loop` prompt construction for the CoderLLM. 1. When a plan step involves Python code generation that is likely to create new functions, methods, or classes, explicitly instruct the CoderLLM in its prompt to include comprehensive docstrings. The instruction should specify that docstrings must explain the purpose, arguments (name, type, description), and return values (type, description). Example instruction: 'IMPORTANT: For any new Python functions, methods, or classes, you MUST include a comprehensive PEP 257 compliant docstring. Use Google-style format (Args:, Returns:, Example: sections). This is required to pass automated ethical and style checks.'. 2. Add unit tests to verify that CoderLLM prompts for Python function/method generation include this instruction in tests/test_phase1_8_features.py.",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_X_fix_multi_target_handling"
      ]
    },
    {
      "task_id": "task_1_8_fix_rate_limit_enh_orchestrator",
      "priority": "Critical",
      "task_name": "Fix Gemini Rate Limiting in EnhancedLLMOrchestrator",
      "description": "The `_call_llm_api` method in `EnhancedLLMOrchestrator` directly calls `_gemini_generate` without going through `_generate_with_retry`, bypassing the rate limiting logic. Modify `_call_llm_api` to correctly incorporate rate limiting for Gemini calls by directly invoking `self._apply_gemini_rate_limit()` before `self._gemini_generate(text)` if the model is 'gemini'. Add unit tests to verify rate limiting is applied in this specific execution path.",
      "status": "Completed",
      "target_file": "src/core/llm_orchestration.py",
      "depends_on": [
        "task_1_8_Y_ensure_docstrings_in_codegen"
      ]
    },
    {
      "task_id": "task_1_8_improve_snippet_handling",
      "priority": "Critical",
      "task_name": "Improve Code Snippet Generation, Validation, and Merging Robustness",
      "description": "The system frequently encounters syntax errors (e.g., 'unterminated string literal', 'unexpected indent') during pre-write validation (`ast.parse`) of CoderLLM-generated snippets or after merging. This blocks autonomous development. This task aims to improve robustness: 1. **Enhanced Logging:** (Completed) Modify `WorkflowDriver._invoke_coder_llm` to save the exact `generated_snippet` string (using `repr()`) to a temporary file if `ast.parse` fails, to capture hidden characters/malformations for debugging. 2. **Prompt Refinement:** (Completed) Review and refine CoderLLM prompts in `workflow_driver.py` to explicitly guide the LLM to output syntactically correct, complete, and context-aware code snippets, minimizing issues with string literals, indentation, and partial outputs. 3. **Merge Strategy Review:** (Completed) Modify the `_merge_snippet` method in `workflow_driver.py` to attempt basic indentation adjustment of the snippet if `METAMORPHIC_INSERT_POINT` is found. 4. **Pre-Merge Full File Syntax Check:** (Completed) Before `_merge_snippet` is called, create a temporary in-memory version of the target file content with the snippet hypothetically inserted at the `METAMORPHIC_INSERT_POINT`. Attempt `ast.parse()` on this *full temporary content*. If this full parse fails, the step should fail with specific feedback indicating an integration syntax error, rather than just the snippet failing.",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, src/core/llm_orchestration.py",
      "depends_on": [
        "task_1_8_fix_rate_limit_enh_orchestrator"
      ]
    },
    {
      "task_id": "task_1_8_improve_snippet_handling_tests",
      "priority": "High",
      "task_name": "Add Unit Tests for Snippet Handling Enhancements",
      "description": "Create comprehensive unit tests for the prompt refinement logic (after refactoring `WorkflowDriver` to expose prompt construction) and the improved `_merge_snippet` method in `src/core/automation/workflow_driver.py`. Tests should cover various scenarios including correct indentation application, handling of empty snippets, and ensuring all new prompt guidelines are present. Place these tests in `tests/test_workflow_driver_enhancements.py`.",
      "status": "Completed",
      "target_file": "tests/test_workflow_driver_enhancements.py",
      "depends_on": [
        "task_1_8_improve_snippet_handling"
      ]
    },
    {
      "task_id": "task_1_8_A_optimize_large_context_epic",
      "priority": "Critical",
      "task_name": "EPIC: Optimize Large Context Handling for Code Generation Steps",
      "description": "OVERARCHING GOAL: Refine `WorkflowDriver.autonomous_loop` and related components to significantly reduce redundant LLM calls when processing large context files for code generation steps, by providing minimal context for simple additions and prompting for only changed lines. This epic is achieved via sub-tasks task_1_8_A_1_helper_is_simple_addition through task_1_8_A_6.",
      "status": "In Progress",
      "target_file": "src/core/automation/workflow_driver.py, src/core/llm_orchestration.py, src/core/chunking/recursive_summarizer.py",
      "depends_on": [
        "task_1_8_Y_ensure_docstrings_in_codegen"
      ]
    },
    {
      "task_id": "task_1_8_A_1_helper_is_simple_addition",
      "priority": "Critical",
      "task_name": "Implement Helper `_is_simple_addition_plan_step`",
      "description": "In `src/core/automation/workflow_driver.py`, implement a new helper method `_is_simple_addition_plan_step(self, plan_step_description: str) -> bool`. This method should analyze the `plan_step_description` string and return `True` if the description indicates a simple, targeted code addition (e.g., adding imports, adding a new method to an *existing* class, adding a constant, appending a line). It should return `False` for complex modifications, refactoring, or new class creation. Ensure comprehensive docstrings and unit tests for this helper in `tests/test_phase1_8_features.py`.",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_Y_ensure_docstrings_in_codegen"
      ]
    },
    {
      "task_id": "task_1_8_A_2_helper_get_context_type",
      "priority": "Critical",
      "task_name": "Implement Helper `_get_context_type_for_step`",
      "description": "In `src/core/automation/workflow_driver.py`, implement `_get_context_type_for_step(self, step_description: str) -> Optional[str]`. This method should analyze the `step_description` and return a string like 'add_import', 'add_method_to_class', 'add_global_function', or None if the type is unclear or complex. Use regex or simple NLP. Ensure comprehensive docstrings and unit tests in `tests/test_phase1_8_features.py`.",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_A_1_helper_is_simple_addition"
      ]
    },
    {
      "task_id": "task_1_8_A_3_helper_extract_targeted_context",
      "priority": "Critical",
      "task_name": "Implement Helper `_extract_targeted_context`",
      "description": "In `src/core/automation/workflow_driver.py`, implement `_extract_targeted_context(self, file_path: str, file_content: str, context_type: Optional[str], step_description: str) -> Tuple[str, bool]`. Based on `context_type` (from task_1_8_A_2), this method should extract the minimal relevant section from `file_content`. For 'add_import', return the import block or top N lines. For 'add_method_to_class', use AST to find the class and return its definition, including existing methods and a few surrounding lines, as context for adding a new method. Return (full_content, False) if context_type is None or extraction fails. Ensure comprehensive docstrings and unit tests in `tests/test_phase1_8_features.py`.",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_A_2_helper_get_context_type"
      ]
    },
    {
      "task_id": "task_1_8_A_4_integrate_minimal_context_logic",
      "priority": "Critical",
      "task_name": "Integrate Minimal Context Logic into WorkflowDriver Loop",
      "description": "Modify `WorkflowDriver.autonomous_loop` in `src/core/automation/workflow_driver.py`. If `_is_simple_addition_plan_step` returns True for the current step: 1. Call `_get_context_type_for_step`. 2. Call `_extract_targeted_context` using the determined type. 3. Pass the extracted context and `is_minimal_context=True` to `_construct_coder_llm_prompt`. If `_is_simple_addition_plan_step` is False, pass full context and `is_minimal_context=False`. Update unit tests.",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_A_3_helper_extract_targeted_context"
      ]
    },
    {
      "task_id": "task_1_8_A_5_refine_coder_prompt_for_minimal_output",
      "priority": "Critical",
      "task_name": "Refine CoderLLM Prompt for Minimal Output on Simple Additions",
      "description": "Modify `WorkflowDriver._construct_coder_llm_prompt` in `src/core/automation/workflow_driver.py`. When `is_minimal_context` is True, add specific instructions to the CoderLLM prompt to output *only* the new/changed lines, not the entire file or section. Reference the `CODER_LLM_MINIMAL_CONTEXT_INSTRUCTION` and `CODER_LLM_TARGETED_MOD_OUTPUT_INSTRUCTIONS` constants. Ensure this is clearly distinct from prompts for full file generation. Update unit tests.",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py, tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_A_4_integrate_minimal_context_logic"
      ]
    },
    {
      "task_id": "task_1_8_A_6_review_summarizer_and_test_e2e",
      "priority": "Critical",
      "task_name": "Review Summarizer Logic and Test End-to-End Context Optimization",
      "description": "1. Review `RecursiveSummarizer.summarize_code_recursively` and `EnhancedLLMOrchestrator._handle_large_context` to ensure they are not triggered unnecessarily for simple additions now that minimal context is provided by `WorkflowDriver`. 2. Perform end-to-end integration testing (manual or new automated test) to confirm that for simple additions to large files, `_handle_large_context` is bypassed and LLM calls are minimized. This validates the success of the preceding sub-tasks.",
      "status": "Not Started",
      "target_file": "src/core/chunking/recursive_summarizer.py, src/core/llm_orchestration.py, tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_A_5_refine_coder_prompt_for_minimal_output"
      ]
    },
    {
      "task_id": "task_1_8_A_test_import_optimization",
      "priority": "Critical",
      "task_name": "Test Import Context Optimization",
      "description": "Create and run an integration test to verify that the context optimization for 'add import' steps in WorkflowDriver is working. The test should: 1. Create a large dummy Python file. 2. Define a plan step to add a new import to this dummy file. 3. Execute this step using a mocked or real LLM. 4. Assert that the context provided to the CoderLLM for the import step was significantly truncated. 5. Assert that the number of summarization/chunk processing calls within EnhancedLLMOrchestrator (if any) is minimal (ideally zero beyond the main CoderLLM call for the snippet). 6. Assert that the import is correctly added to the dummy file. Place the test in tests/test_phase1_8_features.py.",
      "status": "Completed",
      "target_file": "tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_A_6_review_summarizer_and_test_e2e"
      ]
    },
    {
      "task_id": "task_1_8_B_enhance_retry_prompts",
      "priority": "High",
      "task_name": "Enhance Retry Prompts with Specific Validation Feedback",
      "description": "Modify the retry logic within `WorkflowDriver.autonomous_loop`. When a plan step fails due to pre-write validation (syntax, style, ethics) or other captured errors (e.g., test failures post-write), the specific error message(s) should be extracted from `self._current_task_results['pre_write_validation_feedback']` or `self._current_task_results['step_errors']`. This detailed feedback must be appended to the prompt for the CoderLLM during the subsequent retry attempt. The goal is to provide the LLM with concrete information about what went wrong, enabling it to self-correct more effectively.",
      "status": "Completed",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_A_test_import_optimization"
      ]
    },
    {
      "task_id": "task_1_8_2_retry",
      "priority": "Critical",
      "task_name": "Implement Pre-Write Validation per Step (Retry)",
      "description": "Add logic to the Workflow Driver to run basic syntax, style, and ethical checks on generated code snippets *before* merging and writing to the target file. Use CodeReviewAgent and EthicalGovernanceEngine. This is a retry of the original task_1_8_2.",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_B_enhance_retry_prompts"
      ]
    },
    {
      "task_id": "task_1_8_3",
      "priority": "Critical",
      "task_name": "Implement Step-Level Remediation Loop",
      "description": "If pre-write validation fails, provide targeted feedback to the Coder LLM and retry generating the snippet for that specific step (e.g., 2-3 attempts) before failing the step or task.",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_2_retry"
      ]
    },
    {
      "task_id": "task_1_8_4",
      "priority": "Critical",
      "task_name": "Ensure Post-Write, Step-Level Test Execution",
      "description": "Modify the Workflow Driver to automatically trigger relevant tests (using execute_tests) after any plan step that successfully modifies code, especially if the task has a target_file that implies testability (e.g., .py file). Capture results in _current_task_results.",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_3"
      ]
    },
    {
      "task_id": "task_1_8_5",
      "priority": "High",
      "task_name": "Implement Learning from Failures (Data Capture)",
      "description": "Log detailed information about validation failures, remediation attempts, LLM prompts/responses, and outcomes. Store this structured data in the Knowledge Graph or a dedicated database for analysis and learning.",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_4"
      ]
    },
    {
      "task_id": "task_1_8_14",
      "priority": "High",
      "task_name": "Address Ethical Debt in Token Allocator Policy",
      "description": "The temporary fix for AllocationError in EthicalAllocationPolicy bypassed diversity constraints. Implement a robust, ethical token allocation strategy that correctly handles large prompts and multiple chunks, potentially revising Z3 constraints or exploring alternative allocation methods. Ensure the policy promotes model diversity where feasible without causing allocation failures.",
      "status": "Not Started",
      "target_file": "src/core/ethics/constraints.py, src/core/optimization/adaptive_token_allocator.py",
      "depends_on": [
        "task_1_8_A_test_import_optimization",
        "task_1_8_5"
      ]
    },
    {
      "task_id": "task_1_8_7",
      "priority": "Medium",
      "task_name": "Implement Automated Task Decomposition",
      "description": "Add logic to the Workflow Driver or a Planning Agent to assess task complexity and automatically break down large tasks into smaller, dependent sub-tasks in the roadmap if needed.",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_1",
        "task_1_8_5"
      ]
    },
    {
      "task_id": "task_1_8_8",
      "priority": "Medium",
      "task_name": "Refine Grade Report & Error Logging",
      "description": "Update generate_grade_report to clearly distinguish validation execution errors from findings and highlight critical issues. Improve overall logging clarity for debugging autonomous runs.",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_5"
      ]
    },
    {
      "task_id": "task_1_8_9",
      "priority": "High",
      "task_name": "Implement Advanced Code Merging",
      "description": "Replace the current string-based _merge_snippet with an AST-aware merging utility to handle code modifications more robustly and reduce the chance of introducing syntax errors during merging.",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_3"
      ]
    },
    {
      "task_id": "task_1_8_10",
      "priority": "High",
      "task_name": "Implement Prompt Self-Correction Mechanism",
      "description": "If remediation attempts fail for a step, analyze the failure and modify the *prompt* sent to the Coder LLM for the next attempt, or potentially modify the original code generation prompt template based on failure patterns.",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_3",
        "task_1_8_6"
      ]
    },
    {
      "task_id": "task_1_8_11",
      "priority": "High",
      "task_name": "Improve Coder LLM Prompt Generation Logic",
      "description": "Refine how the Workflow Driver constructs prompts for the Coder LLM, incorporating lessons learned from failure data (task_1_8_5) and successful prompt self-correction (task_1_8_10).",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_5",
        "task_1_8_10"
      ]
    },
    {
      "task_id": "task_1_8_12",
      "priority": "Medium",
      "task_name": "Implement Task Success Prediction",
      "description": "Develop a simple model (potentially using data from task_1_8_5) to predict the likelihood of autonomous success for a given task based on its characteristics. Use this to inform task selection or flag low-probability tasks for manual review.",
      "status": "Not Started",
      "target_file": "src/core/automation/workflow_driver.py",
      "depends_on": [
        "task_1_8_5"
      ]
    },
    {
      "task_id": "task_1_8_13",
      "priority": "High",
      "task_name": "Add Comprehensive Tests for Phase 1.8 Features",
      "description": "Write comprehensive unit/integration tests covering all new features in Phase 1.8, including pre-write validation, step-level retries, post-write test execution, improved remediation logic, failure data capture, task decomposition, advanced merging, prompt self-correction, improved prompt generation, and task success prediction.",
      "status": "Not Started",
      "target_file": "tests/test_phase1_8_features.py",
      "depends_on": [
        "task_1_8_1",
        "task_1_8_2_retry",
        "task_1_8_3",
        "task_1_8_4",
        "task_1_8_5",
        "task_1_8_6",
        "task_1_8_7",
        "task_1_8_8",
        "task_1_8_9",
        "task_1_8_10",
        "task_1_8_11",
        "task_1_8_12"
      ]
    },
    {
      "task_id": "task_1_8_C_1_analyze_A1_failure_and_improve_codegen",
      "priority": "High",
      "task_name": "Analyze task_1_8_A_1 Failures & Improve CodeGen",
      "description": "Analyze the failures encountered during the initial autonomous attempt of task_1_8_A_1 (_is_simple_addition_plan_step). Specifically, investigate why the CoderLLM failed to: 1. Correctly include `import re`. 2. Adhere to style guidelines (E302, E501). 3. Consistently include docstrings for new methods. 4. Avoided outputting spurious code that was appended to files. Implement improvements to CoderLLM prompting, snippet cleaning, or pre-write validation to prevent such issues in the future. This task focuses on the AI improving its own code generation process based on past failures.",
      "status": "Not Started",
      "target_file": "src/core/llm_orchestration.py",
      "depends_on": [
        "task_1_8_A_6_review_summarizer_and_test_e2e"
      ]
    }
  ],
  "next_phase_actions": [
    "Set `status`: `Completed` on all Phase 1.8 tasks.",
    "Update the `phase`, `phase_goal`, and `current_focus` fields to 'Phase 2 Iteration 2: Enhanced Agents & Knowledge Graph'."
  ],
  "current_focus": "🎯 CURRENT FOCUS: Phase 1.8 - Hardened Autonomous Loop & Advanced Remediation 🛠️"
}\